{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Article summerizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagen01/AIML/blob/master/Article_summerizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fENdUfwNhkbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating an artical summerizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymiRugzdh3g-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4 as bs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrXMB7jmN5jN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up74vVLAN-Dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the data from source\n",
        "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Optical_character_recognition').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVyNB5QwOiFK",
        "colab_type": "code",
        "outputId": "2e1a43dc-9a3d-49a2-d1c9-5288139dc2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "source"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Optical character recognition - Wikipedia</title>\\n<script>document.documentElement.className=document.documentElement.className.replace(/(^|\\\\s)client-nojs(\\\\s|$)/,\"$1client-js$2\");RLCONF={\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Optical_character_recognition\",\"wgTitle\":\"Optical character recognition\",\"wgCurRevisionId\":912154788,\"wgRevisionId\":912154788,\"wgArticleId\":49091,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 errors: missing periodical\",\"CS1: Julian\\xe2\\x80\\x93Gregorian uncertainty\",\"CS1 errors: deprecated parameters\",\"CS1: long volume value\",\"CS1 maint: multiple names: authors list\",\"EngvarB from January 2019\",\"Articles with short description\",\"Use mdy dates from January 2019\",\"All articles with unsourced statements\",\"Articles with unsourced statements from April 2012\",\"Articles with unsourced statements from October 2011\",\"All articles with vague or ambiguous time\",\"Vague or ambiguous time from March 2013\",\\n\"Wikipedia articles needing clarification from March 2013\",\"Wikipedia articles in need of updating from March 2013\",\"All Wikipedia articles in need of updating\",\"Articles with unsourced statements from May 2009\",\"Commons category link is on Wikidata\",\"Optical character recognition\",\"Artificial intelligence applications\",\"Applications of computer vision\",\"Automatic identification and data capture\",\"Computational linguistics\",\"Unicode\",\"Symbols\",\"Machine learning task\"],\"wgBreakFrames\":!1,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Optical_character_recognition\",\"wgRelevantArticleId\":49091,\"wgRequestId\":\"XW-QBgpAADsAADah2GAAAACS\"\\n,\"wgCSPNonce\":!1,\"wgIsProbablyEditable\":!0,\"wgRelevantPageIsProbablyEditable\":!0,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgMediaViewerOnClick\":!0,\"wgMediaViewerEnabledByDefault\":!0,\"wgPopupsReferencePreviews\":!1,\"wgPopupsConflictsWithNavPopupGadget\":!1,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":!0,\"nearby\":!0,\"watchlist\":!0,\"tagline\":!1},\"wgWMESchemaEditAttemptStepOversample\":!1,\"wgPoweredByHHVM\":!0,\"wgULSCurrentAutonym\":\"English\",\"wgNoticeProject\":\"wikipedia\",\"wgWikibaseItemId\":\"Q167555\",\"wgCentralAuthMobileDomain\":!1,\"wgEditSubmitButtonLabelPublish\":!0};RLSTATE={\"ext.gadget.charinsert-styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"ext.globalCssJs.site.styles\":\"ready\",\"site.styles\":\"ready\",\"noscript\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"ready\",\"user.tokens\":\"loading\",\"ext.cite.styles\":\\n\"ready\",\"ext.tmh.thumbnail.styles\":\"ready\",\"mediawiki.legacy.shared\":\"ready\",\"mediawiki.legacy.commonPrint\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"mediawiki.toc.styles\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.3d.styles\":\"ready\",\"mediawiki.skinning.interface\":\"ready\",\"skins.vector.styles\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"mw.MediaWikiPlayer.loader\",\"mw.PopUpMediaTransform\",\"mw.TMHGalleryHook.js\",\"site\",\"mediawiki.page.startup\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.watchlist-notice\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.globalCssJs.site\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.popups\",\\n\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.cx.eventlogging.campaigns\",\"ext.quicksurveys.init\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"skins.vector.js\"];</script>\\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement(\"user.tokens@0tffind\",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({\"editToken\":\"+\\\\\\\\\",\"patrolToken\":\"+\\\\\\\\\",\"watchToken\":\"+\\\\\\\\\",\"csrfToken\":\"+\\\\\\\\\"});\\n});});</script>\\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.tmh.thumbnail.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\"/>\\n<script async=\"\" src=\"/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector\"></script>\\n<meta name=\"ResourceLoaderDynamicStyles\" content=\"\"/>\\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector\"/>\\n<link rel=\"stylesheet\" href=\"/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\"/>\\n<meta name=\"generator\" content=\"MediaWiki 1.34.0-wmf.20\"/>\\n<meta name=\"referrer\" content=\"origin\"/>\\n<meta name=\"referrer\" content=\"origin-when-crossorigin\"/>\\n<meta name=\"referrer\" content=\"origin-when-cross-origin\"/>\\n<meta property=\"og:image\" content=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Portable_scanner_and_OCR_%28video%29.webm/1200px--Portable_scanner_and_OCR_%28video%29.webm.jpg\"/>\\n<link rel=\"alternate\" href=\"android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Optical_character_recognition\"/>\\n<link rel=\"alternate\" type=\"application/x-wiki\" title=\"Edit this page\" href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit\"/>\\n<link rel=\"edit\" title=\"Edit this page\" href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit\"/>\\n<link rel=\"apple-touch-icon\" href=\"/static/apple-touch/wikipedia.png\"/>\\n<link rel=\"shortcut icon\" href=\"/static/favicon/wikipedia.ico\"/>\\n<link rel=\"search\" type=\"application/opensearchdescription+xml\" href=\"/w/opensearch_desc.php\" title=\"Wikipedia (en)\"/>\\n<link rel=\"EditURI\" type=\"application/rsd+xml\" href=\"//en.wikipedia.org/w/api.php?action=rsd\"/>\\n<link rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\"/>\\n<link rel=\"canonical\" href=\"https://en.wikipedia.org/wiki/Optical_character_recognition\"/>\\n<link rel=\"dns-prefetch\" href=\"//login.wikimedia.org\"/>\\n<link rel=\"dns-prefetch\" href=\"//meta.wikimedia.org\" />\\n<!--[if lt IE 9]><script src=\"/w/resources/lib/html5shiv/html5shiv.js\"></script><![endif]-->\\n</head>\\n<body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Optical_character_recognition rootpage-Optical_character_recognition skin-vector action-view\">\\n<div id=\"mw-page-base\" class=\"noprint\"></div>\\n<div id=\"mw-head-base\" class=\"noprint\"></div>\\n<div id=\"content\" class=\"mw-body\" role=\"main\">\\n\\t<a id=\"top\"></a>\\n\\t<div id=\"siteNotice\" class=\"mw-body-content\"><!-- CentralNotice --></div>\\n\\t<div class=\"mw-indicators mw-body-content\">\\n</div>\\n\\n\\t<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Optical character recognition</h1>\\n\\t\\n\\t<div id=\"bodyContent\" class=\"mw-body-content\">\\n\\t\\t<div id=\"siteSub\" class=\"noprint\">From Wikipedia, the free encyclopedia</div>\\n\\t\\t<div id=\"contentSub\"></div>\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t<div id=\"jump-to-nav\"></div>\\n\\t\\t<a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\\n\\t\\t<a class=\"mw-jump-link\" href=\"#p-search\">Jump to search</a>\\n\\t\\t<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\"><div class=\"mw-parser-output\"><div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Computer recognition of visual text</div>\\n<p class=\"mw-empty-elt\">\\n</p>\\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:302px;\"><div id=\"mwe_player_0\" class=\"PopUpMediaTransform\" style=\"width:300px;\" videopayload=\"&lt;div class=&quot;mediaContainer&quot; style=&quot;width:854px&quot;&gt;&lt;video id=&quot;mwe_player_1&quot; poster=&quot;//upload.wikimedia.org/wikipedia/commons/thumb/8/81/Portable_scanner_and_OCR_%28video%29.webm/854px--Portable_scanner_and_OCR_%28video%29.webm.jpg&quot; controls=&quot;&quot; preload=&quot;none&quot; autoplay=&quot;&quot; style=&quot;width:854px;height:480px&quot; class=&quot;kskin&quot; data-durationhint=&quot;29.403&quot; data-startoffset=&quot;0&quot; data-mwtitle=&quot;Portable_scanner_and_OCR_(video).webm&quot; data-mwprovider=&quot;wikimediacommons&quot;&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.480p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;SD VP9 (480P)&quot; data-shorttitle=&quot;VP9 480P&quot; data-transcodekey=&quot;480p.vp9.webm&quot; data-width=&quot;854&quot; data-height=&quot;480&quot; data-bandwidth=&quot;1017584&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.480p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;SD WebM (480P)&quot; data-shorttitle=&quot;WebM 480P&quot; data-transcodekey=&quot;480p.webm&quot; data-width=&quot;854&quot; data-height=&quot;480&quot; data-bandwidth=&quot;1083992&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.720p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;HD VP9 (720P)&quot; data-shorttitle=&quot;VP9 720P&quot; data-transcodekey=&quot;720p.vp9.webm&quot; data-width=&quot;1280&quot; data-height=&quot;720&quot; data-bandwidth=&quot;2008864&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.720p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;HD WebM (720P)&quot; data-shorttitle=&quot;WebM 720P&quot; data-transcodekey=&quot;720p.webm&quot; data-width=&quot;1280&quot; data-height=&quot;720&quot; data-bandwidth=&quot;2187712&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/8/81/Portable_scanner_and_OCR_%28video%29.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Original WebM file, 1,920 \\xc3\\x97 1,080 (4.12 Mbps)&quot; data-shorttitle=&quot;WebM source&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot; data-bandwidth=&quot;4115823&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.1080p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Full HD WebM (1080P)&quot; data-shorttitle=&quot;WebM 1080P&quot; data-transcodekey=&quot;1080p.webm&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot; data-bandwidth=&quot;4278584&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.1080p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Full HD VP9 (1080P)&quot; data-shorttitle=&quot;VP9 1080P&quot; data-transcodekey=&quot;1080p.vp9.webm&quot; data-width=&quot;1920&quot; data-height=&quot;1080&quot; data-bandwidth=&quot;4871344&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.120p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Lowest bandwidth VP9 (120P)&quot; data-shorttitle=&quot;VP9 120P&quot; data-transcodekey=&quot;120p.vp9.webm&quot; data-width=&quot;214&quot; data-height=&quot;120&quot; data-bandwidth=&quot;178616&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.160p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Low bandwidth WebM (160P)&quot; data-shorttitle=&quot;WebM 160P&quot; data-transcodekey=&quot;160p.webm&quot; data-width=&quot;284&quot; data-height=&quot;160&quot; data-bandwidth=&quot;226592&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.180p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Low bandwidth VP9 (180P)&quot; data-shorttitle=&quot;VP9 180P&quot; data-transcodekey=&quot;180p.vp9.webm&quot; data-width=&quot;320&quot; data-height=&quot;180&quot; data-bandwidth=&quot;250416&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.240p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;Small WebM (240P)&quot; data-shorttitle=&quot;WebM 240P&quot; data-transcodekey=&quot;240p.webm&quot; data-width=&quot;426&quot; data-height=&quot;240&quot; data-bandwidth=&quot;327200&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.240p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;Small VP9 (240P)&quot; data-shorttitle=&quot;VP9 240P&quot; data-transcodekey=&quot;240p.vp9.webm&quot; data-width=&quot;426&quot; data-height=&quot;240&quot; data-bandwidth=&quot;357808&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.360p.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp8, vorbis&amp;quot;&quot; data-title=&quot;WebM (360P)&quot; data-shorttitle=&quot;WebM 360P&quot; data-transcodekey=&quot;360p.webm&quot; data-width=&quot;640&quot; data-height=&quot;360&quot; data-bandwidth=&quot;574576&quot; data-framerate=&quot;25&quot;/&gt;&lt;source src=&quot;//upload.wikimedia.org/wikipedia/commons/transcoded/8/81/Portable_scanner_and_OCR_%28video%29.webm/Portable_scanner_and_OCR_%28video%29.webm.360p.vp9.webm&quot; type=&quot;video/webm; codecs=&amp;quot;vp9, opus&amp;quot;&quot; data-title=&quot;VP9 (360P)&quot; data-shorttitle=&quot;VP9 360P&quot; data-transcodekey=&quot;360p.vp9.webm&quot; data-width=&quot;640&quot; data-height=&quot;360&quot; data-bandwidth=&quot;622968&quot; data-framerate=&quot;25&quot;/&gt;&lt;/video&gt;&lt;/div&gt;\"><img alt=\"File:Portable scanner and OCR (video).webm\" style=\"width:300px;height:169px\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/8/81/Portable_scanner_and_OCR_%28video%29.webm/300px--Portable_scanner_and_OCR_%28video%29.webm.jpg\" /><a href=\"//upload.wikimedia.org/wikipedia/commons/8/81/Portable_scanner_and_OCR_%28video%29.webm\" title=\"Play media\" target=\"new\"><span class=\"play-btn-large\"><span class=\"mw-tmh-playtext\">Play media</span></span></a></div>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Portable_scanner_and_OCR_(video).webm\" class=\"internal\" title=\"Enlarge\"></a></div>Video of the process of scanning and real-time optical character recognition (OCR) with a portable scanner.</div></div></div>\\n<p><b>Optical character recognition</b> or <b>optical character reader</b> (<b>OCR</b>) is the <a href=\"/wiki/Machine\" title=\"Machine\">mechanical</a> or <a href=\"/wiki/Electronics\" title=\"Electronics\">electronic</a> conversion of <a href=\"/wiki/Image\" title=\"Image\">images</a> of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast).<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup>\\n</p><p>Widely used as a form of information entry from printed paper data records \\xe2\\x80\\x93 whether passport documents, invoices, <a href=\"/wiki/Bank_statement\" title=\"Bank statement\">bank statements</a>, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation \\xe2\\x80\\x93 it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as <a href=\"/wiki/Cognitive_computing\" title=\"Cognitive computing\">cognitive computing</a>, <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a>, (extracted) <a href=\"/wiki/Text-to-speech\" class=\"mw-redirect\" title=\"Text-to-speech\">text-to-speech</a>, key data and <a href=\"/wiki/Text_mining\" title=\"Text mining\">text mining</a>. OCR is a field of research in <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a>, <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> and <a href=\"/wiki/Computer_vision\" title=\"Computer vision\">computer vision</a>.\\n</p><p>Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.\\n</p>\\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\\n<ul>\\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#History\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">History</span></a>\\n<ul>\\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Blind_and_visually_impaired_users\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Blind and visually impaired users</span></a></li>\\n</ul>\\n</li>\\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Applications\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Applications</span></a></li>\\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Types\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Types</span></a></li>\\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Techniques\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Techniques</span></a>\\n<ul>\\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Pre-processing\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Pre-processing</span></a></li>\\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Character_recognition\"><span class=\"tocnumber\">4.2</span> <span class=\"toctext\">Character recognition</span></a></li>\\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Post-processing\"><span class=\"tocnumber\">4.3</span> <span class=\"toctext\">Post-processing</span></a></li>\\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Application-specific_optimizations\"><span class=\"tocnumber\">4.4</span> <span class=\"toctext\">Application-specific optimizations</span></a></li>\\n</ul>\\n</li>\\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#Workarounds\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Workarounds</span></a>\\n<ul>\\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Forcing_better_input\"><span class=\"tocnumber\">5.1</span> <span class=\"toctext\">Forcing better input</span></a></li>\\n<li class=\"toclevel-2 tocsection-12\"><a href=\"#Crowdsourcing\"><span class=\"tocnumber\">5.2</span> <span class=\"toctext\">Crowdsourcing</span></a></li>\\n</ul>\\n</li>\\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#Accuracy\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Accuracy</span></a></li>\\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Unicode\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Unicode</span></a></li>\\n<li class=\"toclevel-1 tocsection-15\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>\\n<li class=\"toclevel-1 tocsection-16\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>\\n<li class=\"toclevel-1 tocsection-17\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>\\n</ul>\\n</div>\\n\\n<h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=1\" title=\"Edit section: History\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">See also: <a href=\"/wiki/Timeline_of_optical_character_recognition\" title=\"Timeline of optical character recognition\">Timeline of optical character recognition</a></div>\\n<p>Early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind.<sup id=\"cite_ref-Scantz82_3-0\" class=\"reference\"><a href=\"#cite_note-Scantz82-3\">&#91;3&#93;</a></sup> In 1914, <a href=\"/wiki/Emanuel_Goldberg\" title=\"Emanuel Goldberg\">Emanuel Goldberg</a> developed a machine that read characters and converted them into standard telegraph code.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (April 2012)\">citation needed</span></a></i>&#93;</sup> Concurrently, Edmund Fournier d\\'Albe developed the <a href=\"/wiki/Optophone\" title=\"Optophone\">Optophone</a>, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup>\\n</p><p>In the late 1920s and into the 1930s <a href=\"/wiki/Emanuel_Goldberg\" title=\"Emanuel Goldberg\">Emanuel Goldberg</a> developed what he called a \"Statistical Machine\" for searching <a href=\"/wiki/Microform\" title=\"Microform\">microfilm</a> archives using an optical code recognition system. In 1931 he was granted USA Patent number 1,838,389 for the invention. The patent was acquired by <a href=\"/wiki/IBM\" title=\"IBM\">IBM</a>.\\n</p><p><br />\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Blind_and_visually_impaired_users\">Blind and visually impaired users</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=2\" title=\"Edit section: Blind and visually impaired users\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>In 1974, <a href=\"/wiki/Ray_Kurzweil\" title=\"Ray Kurzweil\">Ray Kurzweil</a> started the company Kurzweil Computer Products, Inc. and continued development of omni-<a href=\"/wiki/Typeface\" title=\"Typeface\">font</a> OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s<sup id=\"cite_ref-Scantz82_3-1\" class=\"reference\"><a href=\"#cite_note-Scantz82-3\">&#91;3&#93;</a></sup><sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup>). Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud. This device required the invention of two enabling technologies&#160;&#8211;&#32;the <a href=\"/wiki/Charge-coupled_device\" title=\"Charge-coupled device\">CCD</a> <a href=\"/wiki/Flatbed_scanner\" class=\"mw-redirect\" title=\"Flatbed scanner\">flatbed scanner</a> and the text-to-speech synthesizer. On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the <a href=\"/wiki/National_Federation_of_the_Blind\" title=\"National Federation of the Blind\">National Federation of the Blind</a>.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2011)\">citation needed</span></a></i>&#93;</sup> In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. <a href=\"/wiki/LexisNexis\" title=\"LexisNexis\">LexisNexis</a> was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases. Two years later, Kurzweil sold his company to <a href=\"/wiki/Xerox\" title=\"Xerox\">Xerox</a>, which had an interest in further commercializing paper-to-computer text conversion. Xerox eventually spun it off as <a href=\"/wiki/Scansoft\" class=\"mw-redirect\" title=\"Scansoft\">Scansoft</a>, which merged with <a href=\"/wiki/Nuance_Communications\" title=\"Nuance Communications\">Nuance Communications</a>.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2011)\">citation needed</span></a></i>&#93;</sup> The research group headed by <a href=\"/wiki/A._G._Ramakrishnan\" title=\"A. G. Ramakrishnan\">A. G. Ramakrishnan</a> at the <a href=\"/wiki/Medical_intelligence_and_language_engineering_lab\" title=\"Medical intelligence and language engineering lab\">Medical intelligence and language engineering lab</a>, <a href=\"/wiki/Indian_Institute_of_Science\" title=\"Indian Institute of Science\">Indian Institute of Science</a>, has developed PrintToBraille tool, an open source GUI front end<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup> that can be used by any OCR to convert scanned images of printed books to Braille books.\\n</p><p>In the 2000s, OCR was made available online as a service (WebOCR), in a <a href=\"/wiki/Cloud_computing\" title=\"Cloud computing\">cloud computing</a> environment, and in mobile applications like real-time translation of foreign-language signs on a <a href=\"/wiki/Smartphone\" title=\"Smartphone\">smartphone</a>. With the advent of smart-phones and <a href=\"/wiki/Smartglasses\" title=\"Smartglasses\">smartglasses</a>, OCR can be used in internet connected mobile device applications that extract text captured using the device\\'s camera. These devices that do not have OCR functionality built into the operating system will typically use an OCR <a href=\"/wiki/Application_programming_interface\" title=\"Application programming interface\">API</a> to extract the text from the image file captured and provided by the device.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup><sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup> The OCR API returns the extracted text, along with information about the location of the detected text in the original image back to the device app for further processing (such as text-to-speech) or display.\\n</p><p><a href=\"/wiki/Comparison_of_optical_character_recognition_software\" title=\"Comparison of optical character recognition software\">Various commercial and open source OCR systems</a> are available for most common <a href=\"/wiki/Writing_system\" title=\"Writing system\">writing systems</a>, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=3\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.\\n</p><p>They can be used for:\\n</p>\\n<ul><li><a href=\"/wiki/Data_entry_clerk\" title=\"Data entry clerk\">Data entry</a> for business documents, e.g. <a href=\"/wiki/Check_clearing\" class=\"mw-redirect\" title=\"Check clearing\">check</a>, passport, invoice, bank statement and receipt</li>\\n<li><a href=\"/wiki/Automatic_number_plate_recognition\" class=\"mw-redirect\" title=\"Automatic number plate recognition\">Automatic number plate recognition</a></li>\\n<li>In airports, for passport recognition and <a href=\"/wiki/Information_extraction\" title=\"Information extraction\">information extraction</a></li>\\n<li>Automatic insurance documents key information extraction</li>\\n<li>Extracting business card information into a contact list<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup></li>\\n<li>More quickly make textual versions of printed documents, e.g. <a href=\"/wiki/Book_scanning\" title=\"Book scanning\">book scanning</a> for <a href=\"/wiki/Project_Gutenberg\" title=\"Project Gutenberg\">Project Gutenberg</a></li>\\n<li>Make electronic images of printed documents searchable, e.g. <a href=\"/wiki/Google_Books\" title=\"Google Books\">Google Books</a></li>\\n<li>Converting handwriting in real time to control a computer (<a href=\"/wiki/Pen_computing\" title=\"Pen computing\">pen computing</a>)</li>\\n<li>Defeating <a href=\"/wiki/CAPTCHA\" title=\"CAPTCHA\">CAPTCHA</a> anti-bot systems, though these are specifically designed to prevent OCR.<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup><sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup> The purpose can also be to test the robustness of CAPTCHA anti-bot systems.</li>\\n<li>Assistive technology for blind and visually impaired users</li></ul>\\n<h2><span class=\"mw-headline\" id=\"Types\">Types</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=4\" title=\"Edit section: Types\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<ul><li>Optical character recognition (OCR)&#160;&#8211;&#32;targets typewritten text, one <a href=\"/wiki/Glyph\" title=\"Glyph\">glyph</a> or <a href=\"/wiki/Character_(symbol)\" title=\"Character (symbol)\">character</a> at a time.</li>\\n<li>Optical word recognition&#160;&#8211;&#32;targets typewritten text, one word at a time (for languages that use a <a href=\"/wiki/Space_(punctuation)\" title=\"Space (punctuation)\">space</a> as a <a href=\"/wiki/Word_divider\" title=\"Word divider\">word divider</a>).  (Usually just called \"OCR\".)</li>\\n<li><a href=\"/wiki/Intelligent_character_recognition\" title=\"Intelligent character recognition\">Intelligent character recognition</a> (ICR)&#160;&#8211;&#32;also targets handwritten <a href=\"/wiki/Printscript\" class=\"mw-redirect\" title=\"Printscript\">printscript</a> or <a href=\"/wiki/Cursive\" title=\"Cursive\">cursive</a> text one glyph or character at a time, usually involving <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a>.</li>\\n<li><a href=\"/wiki/Intelligent_word_recognition\" title=\"Intelligent word recognition\">Intelligent word recognition</a> (IWR)&#160;&#8211;&#32;also targets handwritten <a href=\"/wiki/Printscript\" class=\"mw-redirect\" title=\"Printscript\">printscript</a> or <a href=\"/wiki/Cursive\" title=\"Cursive\">cursive</a> text, one word at a time.  This is especially useful for languages where glyphs are not separated in cursive script.</li></ul>\\n<p>OCR is generally an \"offline\" process, which analyses a static document. <a href=\"/wiki/Handwriting_movement_analysis\" title=\"Handwriting movement analysis\">Handwriting movement analysis</a> can be used as input to <a href=\"/wiki/Handwriting_recognition\" title=\"Handwriting recognition\">handwriting recognition</a>.<sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\">&#91;13&#93;</a></sup>  Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which <a href=\"/wiki/Segment_(handwriting)\" title=\"Segment (handwriting)\">segments</a> are drawn, the direction, and the pattern of putting the pen down and lifting it.  This additional information can make the end-to-end process more accurate.  This technology is also known as \"on-line character recognition\", \"dynamic character recognition\", \"real-time character recognition\", and \"intelligent character recognition\".\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Techniques\">Techniques</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=5\" title=\"Edit section: Techniques\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<h3><span class=\"mw-headline\" id=\"Pre-processing\">Pre-processing</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=6\" title=\"Edit section: Pre-processing\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>OCR software often \"pre-processes\" images to improve the chances of successful recognition. Techniques include:<sup id=\"cite_ref-nicomsoft_14-0\" class=\"reference\"><a href=\"#cite_note-nicomsoft-14\">&#91;14&#93;</a></sup>\\n</p>\\n<ul><li>De-<a href=\"/wiki/Skew_(fax)\" title=\"Skew (fax)\">skew</a>&#160;&#8211;&#32;If the document was not aligned properly when scanned, it may need to be tilted a few degrees clockwise or counterclockwise in order to make lines of text perfectly horizontal or vertical.</li>\\n<li><a href=\"/wiki/Despeckle\" class=\"mw-redirect\" title=\"Despeckle\">Despeckle</a>&#160;&#8211;&#32;remove positive and negative spots, smoothing edges</li>\\n<li>Binarisation&#160;&#8211;&#32;Convert an image from color or <a href=\"/wiki/Greyscale\" class=\"mw-redirect\" title=\"Greyscale\">greyscale</a> to black-and-white (called a \"<a href=\"/wiki/Binary_image\" title=\"Binary image\">binary image</a>\" because there are two colors). The task of binarisation is performed as a simple way of separating the text (or any other desired image component) from the background.<sup id=\"cite_ref-Sezgin2004_15-0\" class=\"reference\"><a href=\"#cite_note-Sezgin2004-15\">&#91;15&#93;</a></sup> The task of binarisation itself is necessary since most commercial recognition algorithms work only on binary images since it proves to be simpler to do so.<sup id=\"cite_ref-Gupta2007_16-0\" class=\"reference\"><a href=\"#cite_note-Gupta2007-16\">&#91;16&#93;</a></sup> In addition, the effectiveness of the binarisation step influences to a significant extent the quality of the character recognition stage and the careful decisions are made in the choice of the binarisation employed for a given input image type; since the quality of the binarisation method employed to obtain the binary result depends on the type of the input image (scanned document, scene text image, historical degraded document etc.).<sup id=\"cite_ref-Trier1995_17-0\" class=\"reference\"><a href=\"#cite_note-Trier1995-17\">&#91;17&#93;</a></sup><sup id=\"cite_ref-Milyaev2013_18-0\" class=\"reference\"><a href=\"#cite_note-Milyaev2013-18\">&#91;18&#93;</a></sup></li>\\n<li>Line removal&#160;&#8211;&#32;Cleans up non-glyph boxes and lines</li>\\n<li><a href=\"/wiki/Document_Layout_Analysis\" class=\"mw-redirect\" title=\"Document Layout Analysis\">Layout analysis</a> or \"zoning\"&#160;&#8211;&#32;Identifies columns, paragraphs, captions, etc. as distinct blocks.  Especially important in <a href=\"/wiki/Column_(typography)\" title=\"Column (typography)\">multi-column layouts</a> and <a href=\"/wiki/Table_(information)\" title=\"Table (information)\">tables</a>.</li>\\n<li>Line and word detection&#160;&#8211;&#32;Establishes baseline for word and character shapes, separates words if necessary.</li>\\n<li>Script recognition&#160;&#8211;&#32;In multilingual documents, the script may change at the level of the words and hence, identification of the script is necessary, before the right OCR can be invoked to handle the specific script.<sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\">&#91;19&#93;</a></sup></li>\\n<li>Character isolation or \"segmentation\"&#160;&#8211;&#32;For per-character OCR, multiple characters that are connected due to image artifacts must be separated; single characters that are broken into multiple pieces due to artifacts must be connected.</li>\\n<li>Normalize <a href=\"/wiki/Aspect_ratio\" title=\"Aspect ratio\">aspect ratio</a> and <a href=\"/wiki/Scale_(ratio)\" title=\"Scale (ratio)\">scale</a><sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup></li></ul>\\n<p>Segmentation of <a href=\"/wiki/Fixed-pitch_font\" class=\"mw-redirect\" title=\"Fixed-pitch font\">fixed-pitch fonts</a> is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas.  For <a href=\"/wiki/Proportional_font\" class=\"mw-redirect\" title=\"Proportional font\">proportional fonts</a>, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character.<sup id=\"cite_ref-Tesseract_overview_21-0\" class=\"reference\"><a href=\"#cite_note-Tesseract_overview-21\">&#91;21&#93;</a></sup>\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Character_recognition\">Character recognition</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=7\" title=\"Edit section: Character recognition\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>There are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters.<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\\n</p><p>Matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as \"pattern matching\", \"<a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a>\", or \"<a href=\"/wiki/Digital_image_correlation\" class=\"mw-redirect\" title=\"Digital image correlation\">image correlation</a>\". This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale.  This technique works best with typewritten text and does not work well when new fonts are encountered.  This is the technique the early physical photocell-based OCR implemented, rather directly.\\n</p><p>Feature extraction decomposes glyphs into \"features\" like lines, closed loops, line direction, and line intersections.  The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient. These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes.  General techniques of <a href=\"/wiki/Feature_detection_(computer_vision)\" title=\"Feature detection (computer vision)\">feature detection in computer vision</a> are applicable to this type of OCR, which is commonly seen in \"intelligent\" <a href=\"/wiki/Handwriting_recognition\" title=\"Handwriting recognition\">handwriting recognition</a> and indeed most modern OCR software.<sup id=\"cite_ref-ocrwizard_23-0\" class=\"reference\"><a href=\"#cite_note-ocrwizard-23\">&#91;23&#93;</a></sup> <a href=\"/wiki/Nearest_neighbour_classifiers\" class=\"mw-redirect\" title=\"Nearest neighbour classifiers\">Nearest neighbour classifiers</a> such as the <a href=\"/wiki/K-nearest_neighbors_algorithm\" title=\"K-nearest neighbors algorithm\">k-nearest neighbors algorithm</a> are used to compare image features with stored glyph features and choose the nearest match.<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup>\\n</p><p>Software such as <a href=\"/wiki/CuneiForm_(software)\" title=\"CuneiForm (software)\">Cuneiform</a> and <a href=\"/wiki/Tesseract_(software)\" title=\"Tesseract (software)\">Tesseract</a> use a two-pass approach to character recognition.  The second pass is known as \"adaptive recognition\" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass.  This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g. blurred or faded).<sup id=\"cite_ref-Tesseract_overview_21-1\" class=\"reference\"><a href=\"#cite_note-Tesseract_overview-21\">&#91;21&#93;</a></sup>\\n</p><p>The OCR result can be stored in the standardized <a href=\"/wiki/ALTO_(XML)\" title=\"ALTO (XML)\">ALTO</a> format, a dedicated XML schema maintained by the United States <a href=\"/wiki/Library_of_Congress\" title=\"Library of Congress\">Library of Congress</a>.\\n</p><p>For a list of optical character recognition software see <a href=\"/wiki/Comparison_of_optical_character_recognition_software\" title=\"Comparison of optical character recognition software\">Comparison of optical character recognition software</a>.\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Post-processing\">Post-processing</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=8\" title=\"Edit section: Post-processing\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>OCR accuracy can be increased if the output is constrained by a <a href=\"/wiki/Lexicon\" title=\"Lexicon\">lexicon</a>&#160;&#8211;&#32;a list of words that are allowed to occur in a document.<sup id=\"cite_ref-nicomsoft_14-1\" class=\"reference\"><a href=\"#cite_note-nicomsoft-14\">&#91;14&#93;</a></sup>  This might be, for example, all the words in the English language, or a more technical lexicon for a specific field.  This technique can be problematic if the document contains words not in the lexicon, like <a href=\"/wiki/Proper_noun\" title=\"Proper noun\">proper nouns</a>. Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy.<sup id=\"cite_ref-Tesseract_overview_21-2\" class=\"reference\"><a href=\"#cite_note-Tesseract_overview-21\">&#91;21&#93;</a></sup>\\n</p><p>The output stream may be a <a href=\"/wiki/Plain_text\" title=\"Plain text\">plain text</a> stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated <a href=\"/wiki/PDF\" title=\"PDF\">PDF</a> that includes both the original image of the page and a searchable textual representation.\\n</p><p>\"Near-neighbor analysis\" can make use of <a href=\"/wiki/Co-occurrence\" title=\"Co-occurrence\">co-occurrence</a> frequencies to correct errors, by noting that certain words are often seen together.<sup id=\"cite_ref-explain_25-0\" class=\"reference\"><a href=\"#cite_note-explain-25\">&#91;25&#93;</a></sup>  For example, \"Washington, D.C.\" is generally far more common in English than \"Washington DOC\".\\n</p><p>Knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy.\\n</p><p>The <a href=\"/wiki/Levenshtein_distance\" title=\"Levenshtein distance\">Levenshtein Distance</a> algorithm has also been used in OCR post-processing to further optimize results from an OCR API.<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup>\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Application-specific_optimizations\">Application-specific optimizations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=9\" title=\"Edit section: Application-specific optimizations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>In recent years,<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items\" title=\"Wikipedia:Manual of Style/Dates and numbers\"><span title=\"The time period mentioned near this tag is ambiguous. (March 2013)\">when?</span></a></i>&#93;</sup> the major OCR technology providers began to tweak OCR systems to deal more efficiently with specific types of input. Beyond an application-specific lexicon, better performance may be had by taking into account business rules, standard expression,<sup class=\"noprint Inline-Template\" style=\"margin-left:0.1em; white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\"The text near this tag may need clarification or removal of jargon. (March 2013)\">clarification needed</span></a></i>&#93;</sup> or rich information contained in color images. This strategy is called \"Application-Oriented OCR\" or \"Customized OCR\", and has been applied to OCR of <a href=\"/wiki/License_plate\" class=\"mw-redirect\" title=\"License plate\">license plates</a>, <a href=\"/wiki/Invoice\" title=\"Invoice\">invoices</a>, <a href=\"/wiki/Screenshot\" title=\"Screenshot\">screenshots</a>, <a href=\"/wiki/ID_card\" class=\"mw-redirect\" title=\"ID card\">ID cards</a>, <a href=\"/wiki/Driver_license\" class=\"mw-redirect\" title=\"Driver license\">driver licenses</a>, and <a href=\"/wiki/Automobile_manufacturing\" class=\"mw-redirect\" title=\"Automobile manufacturing\">automobile manufacturing</a>.\\n</p><p><a href=\"/wiki/The_New_York_Times\" title=\"The New York Times\">The New York Times</a> has adapted the OCR technology into a proprietary tool they entitle, <i>Document Helper</i>, that enables their interactive news team to accelerate the processing of documents that need to be reviewed. They note that it enables them to process what amounts to as many as 5,400 pages per hour in preparation for reporters to review the contents.<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\">&#91;27&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Workarounds\">Workarounds</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=10\" title=\"Edit section: Workarounds\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>There are several techniques for solving the problem of character recognition by means other than improved OCR algorithms.\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Forcing_better_input\">Forcing better input</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=11\" title=\"Edit section: Forcing better input\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>Special fonts like <a href=\"/wiki/OCR-A_font\" class=\"mw-redirect\" title=\"OCR-A font\">OCR-A</a>, <a href=\"/wiki/OCR-B\" title=\"OCR-B\">OCR-B</a>, or <a href=\"/wiki/MICR\" class=\"mw-redirect\" title=\"MICR\">MICR</a> fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription in bank check processing. Ironically however, several prominent OCR engines were designed to capture text in popular fonts such as Arial or Times New Roman, and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts. As Google Tesseract can be trained to recognize new fonts, it can recognize OCR-A, OCR-B and MICR fonts<sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\">&#91;28&#93;</a></sup>.\\n</p><p>\"Comb fields\" are pre-printed boxes that encourage humans to write more legibly&#160;&#8211;&#32;one glyph per box.<sup id=\"cite_ref-explain_25-1\" class=\"reference\"><a href=\"#cite_note-explain-25\">&#91;25&#93;</a></sup>  These are often printed in a <a href=\"/wiki/Drop_out_ink\" title=\"Drop out ink\">\"dropout color\"</a> which can be easily removed by the OCR system.<sup id=\"cite_ref-explain_25-2\" class=\"reference\"><a href=\"#cite_note-explain-25\">&#91;25&#93;</a></sup>\\n</p><p><a href=\"/wiki/Palm_OS\" title=\"Palm OS\">Palm OS</a> used a special set of glyphs, known as \"<a href=\"/wiki/Graffiti_(Palm_OS)\" title=\"Graffiti (Palm OS)\">Graffiti</a>\" which are similar to printed English characters but simplified or modified for easier recognition on the platform\\'s computationally limited hardware. Users would need to learn how to write these special glyphs.\\n</p><p>Zone-based OCR restricts the image to a specific part of a document.  This is often referred to as \"Template OCR\".\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Crowdsourcing\">Crowdsourcing</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=12\" title=\"Edit section: Crowdsourcing\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p><a href=\"/wiki/Crowdsourcing\" title=\"Crowdsourcing\">Crowdsourcing</a> humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than is obtained with computers.  Practical systems include the <a href=\"/wiki/Amazon_Mechanical_Turk\" title=\"Amazon Mechanical Turk\">Amazon Mechanical Turk</a> and <a href=\"/wiki/ReCAPTCHA\" title=\"ReCAPTCHA\">reCAPTCHA</a>. The <a href=\"/wiki/National_Library_of_Finland\" title=\"National Library of Finland\">National Library of Finland</a> has developed an online interface for users to correct OCRed texts in the standardized ALTO format.<sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\">&#91;29&#93;</a></sup> Crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms, for example, through the use of <a href=\"/wiki/Tournament_theory\" title=\"Tournament theory\">rank-order tournaments</a>.<sup id=\"cite_ref-30\" class=\"reference\"><a href=\"#cite_note-30\">&#91;30&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Accuracy\">Accuracy</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=13\" title=\"Edit section: Accuracy\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<table class=\"box-Update plainlinks metadata ambox ambox-content ambox-Update\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"Ambox current red.svg\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/42px-Ambox_current_red.svg.png\" decoding=\"async\" width=\"42\" height=\"34\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/63px-Ambox_current_red.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/98/Ambox_current_red.svg/84px-Ambox_current_red.svg.png 2x\" data-file-width=\"360\" data-file-height=\"290\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article needs to be <b>updated</b>.<span class=\"hide-when-compact\"> Please update this article to reflect recent events or newly available information.</span>  <small class=\"date-container\"><i>(<span class=\"date\">March 2013</span>)</i></small></div></td></tr></tbody></table>\\n<p>Commissioned by the <a href=\"/wiki/U.S._Department_of_Energy\" class=\"mw-redirect\" title=\"U.S. Department of Energy\">U.S. Department of Energy</a> (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the <i>Annual Test of OCR Accuracy</i> from 1992 to 1996.<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup>\\n</p><p>Recognition of <a href=\"/wiki/Latin_alphabet\" title=\"Latin alphabet\">Latin-script</a>, typewritten text is still not 100% accurate even where clear imaging is available. One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%;<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup> total accuracy can be achieved by human review or Data Dictionary Authentication. Other areas\\xe2\\x80\\x94including recognition of hand printing, <a href=\"/wiki/Cursive\" title=\"Cursive\">cursive</a> handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)\\xe2\\x80\\x94are still the subject of active research. The <a href=\"/wiki/MNIST_database\" title=\"MNIST database\">MNIST database</a> is commonly used for testing systems\\' ability to recognise handwritten digits.\\n</p><p>Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate. For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters.<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\">&#91;33&#93;</a></sup>\\n</p><p>An example of the difficulties inherent in digitizing old text is the inability of OCR to differentiate between the \"<a href=\"/wiki/Long_s\" title=\"Long s\">long s</a>\" and \"f\" characters.<sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\">&#91;34&#93;</a></sup>\\n</p><p>Web-based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Manual_of_Style/Dates_and_numbers#Chronological_items\" title=\"Wikipedia:Manual of Style/Dates and numbers\"><span title=\"The time period mentioned near this tag is ambiguous. (March 2013)\">when?</span></a></i>&#93;</sup> (see <a href=\"/wiki/Tablet_computer\" title=\"Tablet computer\">Tablet PC history</a>). Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by <a href=\"/wiki/Pen_computing\" title=\"Pen computing\">pen computing</a> software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (May 2009)\">citation needed</span></a></i>&#93;</sup>\\n</p><p>Recognition of <a href=\"/wiki/Cursive\" title=\"Cursive\">cursive text</a> is an active area of research, with recognition rates even lower than that of <a href=\"/wiki/Hand-printed_text\" class=\"mw-redirect\" title=\"Hand-printed text\">hand-printed text</a>. Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information. For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script. Reading the <i>Amount</i> line of a <a href=\"/wiki/Cheque\" title=\"Cheque\">cheque</a> (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly. The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (May 2009)\">citation needed</span></a></i>&#93;</sup> \\n</p><p>Most programs allow users to set \"confidence rates\". This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Unicode\">Unicode</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=14\" title=\"Edit section: Unicode\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Optical_Character_Recognition_(Unicode_block)\" title=\"Optical Character Recognition (Unicode block)\">Optical Character Recognition (Unicode block)</a></div>\\n<p>Characters to support OCR were added to the <a href=\"/wiki/Unicode\" title=\"Unicode\">Unicode</a> Standard in June 1993, with the release of version 1.1.\\n</p><p>Some of these characters are mapped from fonts specific to <a href=\"/wiki/MICR\" class=\"mw-redirect\" title=\"MICR\">MICR</a>, <a href=\"/wiki/OCR-A\" title=\"OCR-A\">OCR-A</a> or <a href=\"/wiki/OCR-B\" title=\"OCR-B\">OCR-B</a>.\\n</p>\\n<table border=\"1\" cellspacing=\"0\" cellpadding=\"5\" class=\"wikitable nounderlines\" style=\"border-collapse:collapse;background:#FFFFFF;font-size:large;text-align:center\">\\n<tbody><tr>\\n<td colspan=\"17\" style=\"background:#F8F8F8;font-size:small\"><b><a href=\"/wiki/Optical_Character_Recognition_(Unicode_block)\" title=\"Optical Character Recognition (Unicode block)\">Optical Character Recognition</a></b><sup class=\"reference\" id=\"ref_U2440_as_of_Unicode_version\"><a href=\"#endnote_U2440_as_of_Unicode_version\">[1]</a></sup><sup class=\"reference\" id=\"ref_U2440_grey\"><a href=\"#endnote_U2440_grey\">[2]</a></sup><br /><a rel=\"nofollow\" class=\"external text\" href=\"https://www.unicode.org/charts/PDF/U2440.pdf\">Official Unicode Consortium code chart</a> (PDF)\\n</td></tr>\\n<tr style=\"background:#F8F8F8;font-size:small\">\\n<td style=\"width:45pt\">&#160;</td>\\n<td style=\"width:20pt\">0</td>\\n<td style=\"width:20pt\">1</td>\\n<td style=\"width:20pt\">2</td>\\n<td style=\"width:20pt\">3</td>\\n<td style=\"width:20pt\">4</td>\\n<td style=\"width:20pt\">5</td>\\n<td style=\"width:20pt\">6</td>\\n<td style=\"width:20pt\">7</td>\\n<td style=\"width:20pt\">8</td>\\n<td style=\"width:20pt\">9</td>\\n<td style=\"width:20pt\">A</td>\\n<td style=\"width:20pt\">B</td>\\n<td style=\"width:20pt\">C</td>\\n<td style=\"width:20pt\">D</td>\\n<td style=\"width:20pt\">E</td>\\n<td style=\"width:20pt\">F\\n</td></tr>\\n<tr>\\n<td style=\"background:#F8F8F8;font-size:small\">U+244x\\n</td>\\n<td title=\"U+2440: OCR HOOK\">&#x2440;\\n</td>\\n<td title=\"U+2441: OCR CHAIR\">&#x2441;\\n</td>\\n<td title=\"U+2442: OCR FORK\">&#x2442;\\n</td>\\n<td title=\"U+2443: OCR INVERTED FORK\">&#x2443;\\n</td>\\n<td title=\"U+2444: OCR BELT BUCKLE\">&#x2444;\\n</td>\\n<td title=\"U+2445: OCR BOW TIE\">&#x2445;\\n</td>\\n<td title=\"U+2446: OCR BRANCH BANK IDENTIFICATION\">&#x2446;\\n</td>\\n<td title=\"U+2447: OCR AMOUNT OF CHECK\">&#x2447;\\n</td>\\n<td title=\"U+2448: OCR DASH (alias MICR ON US SYMBOL)\">&#x2448;\\n</td>\\n<td title=\"U+2449: OCR CUSTOMER ACCOUNT NUMBER (alias MICR DASH SYMBOL)\">&#x2449;\\n</td>\\n<td title=\"U+244A: OCR DOUBLE BACKSLASH\">&#x244a;\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td></tr>\\n<tr>\\n<td style=\"background:#F8F8F8;font-size:small\">U+245x\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td>\\n<td title=\"Reserved\" style=\"background-color:#CCCCCC;\">\\n</td></tr>\\n<tr>\\n<td colspan=\"17\" style=\"background:#F8F8F8;font-size:small;text-align:left\"><b>Notes</b>\\n<dl><dd>1.<span class=\"citation wikicite\" id=\"endnote_U2440_as_of_Unicode_version\"><b><a href=\"#ref_U2440_as_of_Unicode_version\">^</a></b></span> As of Unicode version 12.0</dd>\\n<dd>2.<span class=\"citation wikicite\" id=\"endnote_U2440_grey\"><b><a href=\"#ref_U2440_grey\">^</a></b></span> Grey areas indicate non-assigned code points</dd></dl>\\n</td></tr></tbody></table>\\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=15\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"div-col columns column-width\" style=\"-moz-column-width: 22em; -webkit-column-width: 22em; column-width: 22em;\">\\n<ul><li><a href=\"/wiki/AI_effect\" title=\"AI effect\">AI effect</a></li>\\n<li><a href=\"/wiki/Applications_of_artificial_intelligence\" title=\"Applications of artificial intelligence\">Applications of artificial intelligence</a></li>\\n<li><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></li>\\n<li><a href=\"/wiki/Digital_library\" title=\"Digital library\">Digital library</a></li>\\n<li><a href=\"/wiki/Digital_mailroom\" title=\"Digital mailroom\">Digital mailroom</a></li>\\n<li><a href=\"/wiki/Digital_pen\" title=\"Digital pen\">Digital pen</a></li>\\n<li><a href=\"/wiki/Institutional_repository\" title=\"Institutional repository\">Institutional repository</a></li>\\n<li><a href=\"/wiki/Legibility\" title=\"Legibility\">Legibility</a></li>\\n<li><a href=\"/wiki/List_of_emerging_technologies\" title=\"List of emerging technologies\">List of emerging technologies</a></li>\\n<li><a href=\"/wiki/Live_ink_character_recognition_solution\" class=\"mw-redirect\" title=\"Live ink character recognition solution\">Live ink character recognition solution</a></li>\\n<li><a href=\"/wiki/Magnetic_ink_character_recognition\" title=\"Magnetic ink character recognition\">Magnetic ink character recognition</a></li>\\n<li><a href=\"/wiki/Music_OCR\" class=\"mw-redirect\" title=\"Music OCR\">Music OCR</a></li>\\n<li><a href=\"/wiki/OCR_in_Indian_Languages\" class=\"mw-redirect\" title=\"OCR in Indian Languages\">OCR in Indian Languages</a></li>\\n<li><a href=\"/wiki/Optical_mark_recognition\" title=\"Optical mark recognition\">Optical mark recognition</a></li>\\n<li><a href=\"/wiki/Outline_of_artificial_intelligence\" title=\"Outline of artificial intelligence\">Outline of artificial intelligence</a></li>\\n<li><a href=\"/wiki/Sketch_recognition\" title=\"Sketch recognition\">Sketch recognition</a></li>\\n<li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\\n<li><a href=\"/wiki/Voice_recording\" class=\"mw-redirect\" title=\"Voice recording\">Voice recording</a></li>\\n<li><a href=\"/wiki/Comparison_of_optical_character_recognition_software\" title=\"Comparison of optical character recognition software\">Comparison of optical character recognition software</a></li>\\n<li><a href=\"/wiki/Tesseract_(software)\" title=\"Tesseract (software)\">Tesseract OCR engine</a></li></ul>\\n</div>\\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=16\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\\n<ol class=\"references\">\\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">OnDemand, HPE Haven. <a rel=\"nofollow\" class=\"external text\" href=\"https://dev.havenondemand.com/apis/ocrdocument#overview\">\"OCR Document\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=OCR+Document&amp;rft.aulast=OnDemand&amp;rft.aufirst=HPE+Haven&amp;rft_id=https%3A%2F%2Fdev.havenondemand.com%2Fapis%2Focrdocument%23overview&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><style data-mw-deduplicate=\"TemplateStyles:r886058088\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\\n</li>\\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">OnDemand, HPE Haven. <a rel=\"nofollow\" class=\"external text\" href=\"https://dev.havenondemand.com/docs/ImageFormats.html\">\"undefined\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=undefined&amp;rft.aulast=OnDemand&amp;rft.aufirst=HPE+Haven&amp;rft_id=https%3A%2F%2Fdev.havenondemand.com%2Fdocs%2FImageFormats.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Scantz82-3\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Scantz82_3-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Scantz82_3-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Schantz, Herbert F. (1982). <i>The history of OCR, optical character recognition</i>. [Manchester Center, Vt.]: Recognition Technologies Users Association. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/9780943072012\" title=\"Special:BookSources/9780943072012\"><bdi>9780943072012</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+history+of+OCR%2C+optical+character+recognition&amp;rft.place=%5BManchester+Center%2C+Vt.%5D&amp;rft.pub=Recognition+Technologies+Users+Association&amp;rft.date=1982&amp;rft.isbn=9780943072012&amp;rft.aulast=Schantz&amp;rft.aufirst=Herbert+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">d\\'Albe, E. E. F. (July 1, 1914). \"On a Type-Reading Optophone\". <i>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</i>. <b>90</b> (619): 373\\xe2\\x80\\x93375. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/1914RSPSA..90..373D\">1914RSPSA..90..373D</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1098%2Frspa.1914.0061\">10.1098/rspa.1914.0061</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Royal+Society+A%3A+Mathematical%2C+Physical+and+Engineering+Sciences&amp;rft.atitle=On+a+Type-Reading+Optophone&amp;rft.volume=90&amp;rft.issue=619&amp;rft.pages=373-375&amp;rft.date=1914-07-01&amp;rft_id=info%3Adoi%2F10.1098%2Frspa.1914.0061&amp;rft_id=info%3Abibcode%2F1914RSPSA..90..373D&amp;rft.aulast=d%27Albe&amp;rft.aufirst=E.+E.+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">\"The History of OCR\". <i>Data Processing Magazine</i>. <b>12</b>: 46. 1970.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Data+Processing+Magazine&amp;rft.atitle=The+History+of+OCR&amp;rft.volume=12&amp;rft.pages=46&amp;rft.date=1970&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">PrintToBraille Tool. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20141225115650/https://code.google.com/p/ocr-gui-frontend/\">\"ocr-gui-frontend\"</a>. MILE Lab, Dept of EE, IISc. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"https://code.google.com/p/ocr-gui-frontend/\">the original</a> on December 25, 2014<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">December 7,</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=ocr-gui-frontend&amp;rft.pub=MILE+Lab%2C+Dept+of+EE%2C+IISc&amp;rft.au=PrintToBraille+Tool&amp;rft_id=https%3A%2F%2Fcode.google.com%2Fp%2Focr-gui-frontend%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite uses deprecated parameter <code class=\"cs1-code\">&#124;deadurl=</code> (<a href=\"/wiki/Help:CS1_errors#deprecated_params\" title=\"Help:CS1 errors\">help</a>); </span><span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://community.havenondemand.com/t5/Blog/Extracting-text-from-images-using-OCR-on-Android/ba-p/1883\">\"Extracting text from images using OCR on Android\"</a>. June 27, 2015.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Extracting+text+from+images+using+OCR+on+Android&amp;rft.date=2015-06-27&amp;rft_id=https%3A%2F%2Fcommunity.havenondemand.com%2Ft5%2FBlog%2FExtracting-text-from-images-using-OCR-on-Android%2Fba-p%2F1883&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://community.havenondemand.com/t5/Blog/Tutorial-OCR-on-Google-Glass/ba-p/1164\">\"&#91;Tutorial&#93; OCR on Google Glass\"</a>. October 23, 2014.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%5BTutorial%5D+OCR+on+Google+Glass&amp;rft.date=2014-10-23&amp;rft_id=https%3A%2F%2Fcommunity.havenondemand.com%2Ft5%2FBlog%2FTutorial-OCR-on-Google-Glass%2Fba-p%2F1164&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://community.havenondemand.com/t5/Blog/javascript-Using-OCR-and-Entity-Extraction-for-LinkedIn-Company/ba-p/460\">\"&#91;javascript&#93; Using OCR and Entity Extraction for LinkedIn Company Lookup\"</a>. July 22, 2014.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%5Bjavascript%5D+Using+OCR+and+Entity+Extraction+for+LinkedIn+Company+Lookup&amp;rft.date=2014-07-22&amp;rft_id=https%3A%2F%2Fcommunity.havenondemand.com%2Ft5%2FBlog%2Fjavascript-Using-OCR-and-Entity-Extraction-for-LinkedIn-Company%2Fba-p%2F460&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.andrewt.net/blog/how-to-crack-captchas/\">\"How To Crack Captchas\"</a>. andrewt.net. June 28, 2006<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+To+Crack+Captchas&amp;rft.pub=andrewt.net&amp;rft.date=2006-06-28&amp;rft_id=http%3A%2F%2Fwww.andrewt.net%2Fblog%2Fhow-to-crack-captchas%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.sfu.ca/~mori/research/gimpy/\">\"Breaking a Visual CAPTCHA\"</a>. Cs.sfu.ca. December 10, 2002<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Breaking+a+Visual+CAPTCHA&amp;rft.pub=Cs.sfu.ca&amp;rft.date=2002-12-10&amp;rft_id=http%3A%2F%2Fwww.cs.sfu.ca%2F~mori%2Fresearch%2Fgimpy%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">John Resig (January 23, 2009). <a rel=\"nofollow\" class=\"external text\" href=\"http://ejohn.org/blog/ocr-and-neural-nets-in-javascript/\">\"John Resig \\xe2\\x80\\x93 OCR and Neural Nets in JavaScript\"</a>. Ejohn.org<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=John+Resig+%E2%80%93+OCR+and+Neural+Nets+in+JavaScript&amp;rft.pub=Ejohn.org&amp;rft.date=2009-01-23&amp;rft.au=John+Resig&amp;rft_id=http%3A%2F%2Fejohn.org%2Fblog%2Focr-and-neural-nets-in-javascript%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Tappert, C. C.; Suen, C. Y.; Wakahara, T. (1990). \"The state of the art in online handwriting recognition\". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>12</b> (8): 787. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109%2F34.57669\">10.1109/34.57669</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=The+state+of+the+art+in+online+handwriting+recognition&amp;rft.volume=12&amp;rft.issue=8&amp;rft.pages=787&amp;rft.date=1990&amp;rft_id=info%3Adoi%2F10.1109%2F34.57669&amp;rft.aulast=Tappert&amp;rft.aufirst=C.+C.&amp;rft.au=Suen%2C+C.+Y.&amp;rft.au=Wakahara%2C+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-nicomsoft-14\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-nicomsoft_14-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-nicomsoft_14-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.nicomsoft.com/optical-character-recognition-ocr-how-it-works/\">\"Optical Character Recognition (OCR) \\xe2\\x80\\x93 How it works\"</a>. Nicomsoft.com<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Optical+Character+Recognition+%28OCR%29+%E2%80%93+How+it+works&amp;rft.pub=Nicomsoft.com&amp;rft_id=https%3A%2F%2Fwww.nicomsoft.com%2Foptical-character-recognition-ocr-how-it-works%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Sezgin2004-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Sezgin2004_15-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sezgin, Mehmet; Sankur, Bulent (2004). <a rel=\"nofollow\" class=\"external text\" href=\"http://webdocs.cs.ualberta.ca/~nray1/CMPUT605/track3_papers/Threshold_survey.pdf\">\"Survey over image thresholding techniques and quantitative performance evaluation\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Journal of Electronic Imaging</i>. <b>13</b> (1): 146. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2004JEI....13..146S\">2004JEI....13..146S</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1117%2F1.1631315\">10.1117/1.1631315</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 2,</span> 2015</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Electronic+Imaging&amp;rft.atitle=Survey+over+image+thresholding+techniques+and+quantitative+performance+evaluation&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=146&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1117%2F1.1631315&amp;rft_id=info%3Abibcode%2F2004JEI....13..146S&amp;rft.aulast=Sezgin&amp;rft.aufirst=Mehmet&amp;rft.au=Sankur%2C+Bulent&amp;rft_id=http%3A%2F%2Fwebdocs.cs.ualberta.ca%2F~nray1%2FCMPUT605%2Ftrack3_papers%2FThreshold_survey.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Gupta2007-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Gupta2007_16-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Gupta, Maya R.; Jacobson, Nathaniel P.; Garcia, Eric K. (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.rfai.li.univ-tours.fr/fr/ressources/_dh/DOC/DocOCR/OCRbinarisation.pdf\">\"OCR binarisation and image pre-processing for searching historical documents\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Pattern Recognition</i>. <b>40</b> (2): 389. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016%2Fj.patcog.2006.04.043\">10.1016/j.patcog.2006.04.043</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 2,</span> 2015</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition&amp;rft.atitle=OCR+binarisation+and+image+pre-processing+for+searching+historical+documents.&amp;rft.volume=40&amp;rft.issue=2&amp;rft.pages=389&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2006.04.043&amp;rft.aulast=Gupta&amp;rft.aufirst=Maya+R.&amp;rft.au=Jacobson%2C+Nathaniel+P.&amp;rft.au=Garcia%2C+Eric+K.&amp;rft_id=http%3A%2F%2Fwww.rfai.li.univ-tours.fr%2Ffr%2Fressources%2F_dh%2FDOC%2FDocOCR%2FOCRbinarisation.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Trier1995-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Trier1995_17-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Trier, Oeivind Due; Jain, Anil K. (1995). <a rel=\"nofollow\" class=\"external text\" href=\"http://heim.ifi.uio.no/inf386/trier2.pdf\">\"Goal-directed evaluation of binarisation methods\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>17</b> (12): 1191\\xe2\\x80\\x931201. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109%2F34.476511\">10.1109/34.476511</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 2,</span> 2015</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=Goal-directed+evaluation+of+binarisation+methods.&amp;rft.volume=17&amp;rft.issue=12&amp;rft.pages=1191-1201&amp;rft.date=1995&amp;rft_id=info%3Adoi%2F10.1109%2F34.476511&amp;rft.aulast=Trier&amp;rft.aufirst=Oeivind+Due&amp;rft.au=Jain%2C+Anil+K.&amp;rft_id=http%3A%2F%2Fheim.ifi.uio.no%2Finf386%2Ftrier2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Milyaev2013-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Milyaev2013_18-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Milyaev, Sergey; Barinova, Olga; Novikova, Tatiana; Kohli, Pushmeet; Lempitsky, Victor (2013). <a rel=\"nofollow\" class=\"external text\" href=\"http://research.microsoft.com/en-us/um/people/pkohli/papers/mbnlk_icdar2013.pdf\">\"Image binarisation for end-to-end text understanding in natural images\"</a> <span class=\"cs1-format\">(PDF)</span>. <i>Document Analysis and Recognition (ICDAR) 2013</i>. 12th International Conference on<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 2,</span> 2015</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Document+Analysis+and+Recognition+%28ICDAR%29+2013&amp;rft.atitle=Image+binarisation+for+end-to-end+text+understanding+in+natural+images.&amp;rft.volume=12th+International+Conference+on&amp;rft.date=2013&amp;rft.aulast=Milyaev&amp;rft.aufirst=Sergey&amp;rft.au=Barinova%2C+Olga&amp;rft.au=Novikova%2C+Tatiana&amp;rft.au=Kohli%2C+Pushmeet&amp;rft.au=Lempitsky%2C+Victor&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fpkohli%2Fpapers%2Fmbnlk_icdar2013.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Pati, P.B.; Ramakrishnan, A.G. (May 29, 1987). \"Word Level Multi-script Identification\". <i>Pattern Recognition Letters</i>. <b>29</b> (9): 1218\\xe2\\x80\\x931229. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016%2Fj.patrec.2008.01.027\">10.1016/j.patrec.2008.01.027</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Pattern+Recognition+Letters&amp;rft.atitle=Word+Level+Multi-script+Identification&amp;rft.volume=29&amp;rft.issue=9&amp;rft.pages=1218-1229&amp;rft.date=1987-05-29&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patrec.2008.01.027&amp;rft.aulast=Pati&amp;rft.aufirst=P.B.&amp;rft.au=Ramakrishnan%2C+A.G.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://blog.damiles.com/2008/11/20/basic-ocr-in-opencv.html\">\"Basic OCR in OpenCV &#124; Damiles\"</a>. Blog.damiles.com. November 20, 2008<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Basic+OCR+in+OpenCV+%26%23124%3B+Damiles&amp;rft.pub=Blog.damiles.com&amp;rft.date=2008-11-20&amp;rft_id=http%3A%2F%2Fblog.damiles.com%2F2008%2F11%2F20%2Fbasic-ocr-in-opencv.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-Tesseract_overview-21\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Tesseract_overview_21-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Tesseract_overview_21-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Tesseract_overview_21-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\">Ray Smith (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://tesseract-ocr.googlecode.com/svn/trunk/doc/tesseracticdar2007.pdf\">\"An Overview of the Tesseract OCR Engine\"</a> <span class=\"cs1-format\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">May 23,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=An+Overview+of+the+Tesseract+OCR+Engine&amp;rft.date=2007&amp;rft.au=Ray+Smith&amp;rft_id=http%3A%2F%2Ftesseract-ocr.googlecode.com%2Fsvn%2Ftrunk%2Fdoc%2Ftesseracticdar2007.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.dataid.com/aboutocr.htm\">\"OCR Introduction\"</a>. Dataid.com<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=OCR+Introduction&amp;rft.pub=Dataid.com&amp;rft_id=http%3A%2F%2Fwww.dataid.com%2Faboutocr.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-ocrwizard-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ocrwizard_23-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://ocrwizard.com/ocr-software/how-ocr-software-works.html\">\"How OCR Software Works\"</a>. OCRWizard<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+OCR+Software+Works&amp;rft.pub=OCRWizard&amp;rft_id=http%3A%2F%2Focrwizard.com%2Focr-software%2Fhow-ocr-software-works.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://blog.damiles.com/2008/11/14/the-basic-patter-recognition-and-classification-with-opencv.html\">\"The basic pattern recognition and classification with openCV &#124; Damiles\"</a>. Blog.damiles.com. November 14, 2008<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+basic+pattern+recognition+and+classification+with+openCV+%26%23124%3B+Damiles&amp;rft.pub=Blog.damiles.com&amp;rft.date=2008-11-14&amp;rft_id=http%3A%2F%2Fblog.damiles.com%2F2008%2F11%2F14%2Fthe-basic-patter-recognition-and-classification-with-opencv.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-explain-25\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-explain_25-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-explain_25-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-explain_25-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.explainthatstuff.com/how-ocr-works.html\">\"How does OCR document scanning work?\"</a>. Explain that Stuff. January 30, 2012<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">June 16,</span> 2013</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+does+OCR+document+scanning+work%3F&amp;rft.pub=Explain+that+Stuff&amp;rft.date=2012-01-30&amp;rft_id=http%3A%2F%2Fwww.explainthatstuff.com%2Fhow-ocr-works.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://community.havenondemand.com/t5/Wiki/How-to-optimize-results-from-the-OCR-API-when-extracting-text/ta-p/1656\">\"How to optimize results from the OCR API when extracting text from an image? - Haven OnDemand Developer Community\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+to+optimize+results+from+the+OCR+API+when+extracting+text+from+an+image%3F+-+Haven+OnDemand+Developer+Community&amp;rft_id=https%3A%2F%2Fcommunity.havenondemand.com%2Ft5%2FWiki%2FHow-to-optimize-results-from-the-OCR-API-when-extracting-text%2Fta-p%2F1656&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\">Fehr, Tiff, <i><a rel=\"nofollow\" class=\"external text\" href=\"https://www.nytimes.com/2019/03/26/reader-center/times-documents-reporters-cohen.html?rref=collection%2Fsectioncollection%2Freader-center&amp;action=click&amp;contentCollection=reader-center&amp;region=rank&amp;module=package&amp;version=highlights&amp;contentPlacement=2&amp;pgtype=sectionfront\">How We Sped Through 900 Pages of Cohen Documents in Under 10 Minutes</a></i>, Times Insider, <a href=\"/wiki/The_New_York_Times\" title=\"The New York Times\">The New York Times</a>, March 26, 2019</span>\\n</li>\\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://trainyourtesseract.com/\">\"Train Your Tesseract\"</a>. <i>Train Your Tesseract</i>. September 20, 2018<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 20,</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Train+Your+Tesseract&amp;rft.atitle=Train+Your+Tesseract&amp;rft.date=2018-09-20&amp;rft_id=http%3A%2F%2Ftrainyourtesseract.com%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://blogs.helsinki.fi/fennougrica/2014/02/21/ocr-text-editor/\">\"What is the point of an online interactive OCR text editor? - Fenno-Ugrica\"</a>. February 21, 2014.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=What+is+the+point+of+an+online+interactive+OCR+text+editor%3F+-+Fenno-Ugrica&amp;rft.date=2014-02-21&amp;rft_id=http%3A%2F%2Fblogs.helsinki.fi%2Ffennougrica%2F2014%2F02%2F21%2Focr-text-editor%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Riedl, C.; Zanibbi, R.; Hearst, M. A.; Zhu, S.; Menietti, M.; Crusan, J.; Metelsky, I.; Lakhani, K. (February 20, 2016). \"Detecting Figures and Part Labels in Patents: Competition-Based Development of Image Processing Algorithms\". <i><a href=\"/w/index.php?title=International_Journal_on_Document_Analysis_and_Recognition&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"International Journal on Document Analysis and Recognition (page does not exist)\">International Journal on Document Analysis and Recognition</a></i>. <b>19</b> (2): 155. <a href=\"/wiki/ArXiv\" title=\"ArXiv\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//arxiv.org/abs/1410.6751\">1410.6751</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1007%2Fs10032-016-0260-8\">10.1007/s10032-016-0260-8</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+on+Document+Analysis+and+Recognition&amp;rft.atitle=Detecting+Figures+and+Part+Labels+in+Patents%3A+Competition-Based+Development+of+Image+Processing+Algorithms&amp;rft.volume=19&amp;rft.issue=2&amp;rft.pages=155&amp;rft.date=2016-02-20&amp;rft_id=info%3Aarxiv%2F1410.6751&amp;rft_id=info%3Adoi%2F10.1007%2Fs10032-016-0260-8&amp;rft.au=Riedl%2C+C.&amp;rft.au=Zanibbi%2C+R.&amp;rft.au=Hearst%2C+M.+A.&amp;rft.au=Zhu%2C+S.&amp;rft.au=Menietti%2C+M.&amp;rft.au=Crusan%2C+J.&amp;rft.au=Metelsky%2C+I.&amp;rft.au=Lakhani%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://code.google.com/p/isri-ocr-evaluation-tools/\">\"Code and Data to evaluate OCR accuracy, originally from UNLV/ISRI\"</a>. Google Code Archive.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Code+and+Data+to+evaluate+OCR+accuracy%2C+originally+from+UNLV%2FISRI&amp;rft.pub=Google+Code+Archive&amp;rft_id=https%3A%2F%2Fcode.google.com%2Fp%2Fisri-ocr-evaluation-tools%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Holley, Rose (April 2009). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.dlib.org/dlib/march09/holley/03holley.html\">\"How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs\"</a>. D-Lib Magazine<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">January 5,</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=How+Good+Can+It+Get%3F+Analysing+and+Improving+OCR+Accuracy+in+Large+Scale+Historic+Newspaper+Digitisation+Programs&amp;rft.pub=D-Lib+Magazine&amp;rft.date=2009-04&amp;rft.aulast=Holley&amp;rft.aufirst=Rose&amp;rft_id=http%3A%2F%2Fwww.dlib.org%2Fdlib%2Fmarch09%2Fholley%2F03holley.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span> <span class=\"cs1-hidden-error error citation-comment\">Cite web requires <code class=\"cs1-code\">&#124;website=</code> (<a href=\"/wiki/Help:CS1_errors#missing_periodical\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Suen, C.Y.; Plamondon, R.; Tappert, A.; Thomassen, A.; Ward, J.R.; Yamamoto, K. (May 29, 1987). <a rel=\"nofollow\" class=\"external text\" href=\"http://users.erols.com/rwservices/pens/biblio88.html#Suen88\"><i>Future Challenges in Handwriting and Computer Applications</i></a>. 3rd International Symposium on Handwriting and Computer Applications, Montreal, May 29, 1987<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">October 3,</span> 2008</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Future+Challenges+in+Handwriting+and+Computer+Applications&amp;rft.date=1987-05-29&amp;rft.aulast=Suen&amp;rft.aufirst=C.Y.&amp;rft.au=Plamondon%2C+R.&amp;rft.au=Tappert%2C+A.&amp;rft.au=Thomassen%2C+A.&amp;rft.au=Ward%2C+J.R.&amp;rft.au=Yamamoto%2C+K.&amp;rft_id=http%3A%2F%2Fusers.erols.com%2Frwservices%2Fpens%2Fbiblio88.html%23Suen88&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Sarantos Kapidakis, Cezary Mazurek, Marcin Werla (2015). <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/?id=kEyGCgAAQBAJ&amp;dq=OCR+and+long+s\"><i>Research and Advanced Technology for Digital Libraries</i></a>. Springer. p.&#160;257. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/9783319245928\" title=\"Special:BookSources/9783319245928\"><bdi>9783319245928</bdi></a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">April 3,</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Research+and+Advanced+Technology+for+Digital+Libraries&amp;rft.pages=257&amp;rft.pub=Springer&amp;rft.date=2015&amp;rft.isbn=9783319245928&amp;rft.au=Sarantos+Kapidakis%2C+Cezary+Mazurek%2C+Marcin+Werla&amp;rft_id=https%3A%2F%2Fbooks.google.com%2F%3Fid%3DkEyGCgAAQBAJ%26dq%3DOCR%2Band%2Blong%2Bs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AOptical+character+recognition\" class=\"Z3988\"></span><span class=\"cs1-maint citation-comment\">CS1 maint: multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">link</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r886058088\"/></span>\\n</li>\\n</ol></div>\\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit&amp;section=17\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<table role=\"presentation\" class=\"mbox-small plainlinks sistersitebox\" style=\"background-color:#f9f9f9;border:1px solid #aaa;color:#000\">\\n<tbody><tr>\\n<td class=\"mbox-image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png\" decoding=\"async\" width=\"30\" height=\"40\" class=\"noviewer\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/45px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/59px-Commons-logo.svg.png 2x\" data-file-width=\"1024\" data-file-height=\"1376\" /></td>\\n<td class=\"mbox-text plainlist\">Wikimedia Commons has media related to <i><b><a href=\"https://commons.wikimedia.org/wiki/Category:Optical_character_recognition\" class=\"extiw\" title=\"commons:Category:Optical character recognition\">Optical character recognition</a></b></i>.</td></tr>\\n</tbody></table>\\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.unicode.org/charts/PDF/U2440.pdf\">Unicode OCR&#160;&#8211;&#32;Hex Range: 2440-245F</a> Optical Character Recognition in Unicode</li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://ruetersward.com/biblio.html\">Annotated bibliography of references to handwriting character recognition and pen computing</a></li></ul>\\n<p><br />\\n</p>\\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Optical_character_recognition_software\" style=\"padding:3px\"><table class=\"nowraplinks mw-collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:OCR\" title=\"Template:OCR\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:OCR\" title=\"Template talk:OCR\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:OCR&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Optical_character_recognition_software\" style=\"font-size:114%;margin:0 4em\"><a class=\"mw-selflink selflink\">Optical character recognition</a> software</div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Free_software\" title=\"Free software\">Free software</a></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/CuneiForm_(software)\" title=\"CuneiForm (software)\">CuneiForm</a></li>\\n<li><a href=\"/wiki/GOCR\" title=\"GOCR\">GOCR</a></li>\\n<li><a href=\"/wiki/Ocrad\" title=\"Ocrad\">Ocrad</a></li>\\n<li><a href=\"/wiki/OCRFeeder\" title=\"OCRFeeder\">OCRFeeder</a></li>\\n<li><a href=\"/wiki/OCRopus\" title=\"OCRopus\">OCRopus</a></li>\\n<li><a href=\"/wiki/Tesseract_(software)\" title=\"Tesseract (software)\">Tesseract</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Proprietary_software\" title=\"Proprietary software\">Proprietary software</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/ABBYY_FineReader\" title=\"ABBYY FineReader\">ABBYY FineReader</a></li>\\n<li><a href=\"/wiki/Asprise_OCR\" title=\"Asprise OCR\">Asprise OCR</a></li>\\n<li><a href=\"/wiki/Microsoft_Office_Document_Imaging\" class=\"mw-redirect\" title=\"Microsoft Office Document Imaging\">Microsoft Office Document Imaging</a></li>\\n<li><a href=\"/wiki/OmniPage\" title=\"OmniPage\">OmniPage</a></li>\\n<li><a href=\"/wiki/ReadSoft\" title=\"ReadSoft\">ReadSoft</a></li>\\n<li><a href=\"/wiki/SmartScore\" title=\"SmartScore\">SmartScore</a></li>\\n<li><a href=\"/wiki/TeleForm\" title=\"TeleForm\">TeleForm</a></li></ul>\\n<ul><li><a href=\"/wiki/VueScan\" title=\"VueScan\">VueScan</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">See also</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Comparison_of_optical_character_recognition_software\" title=\"Comparison of optical character recognition software\">Comparison of optical character recognition software</a></li></ul>\\n</div></td></tr></tbody></table></div>\\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Natural_language_processing\" style=\"padding:3px\"><table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Natural_Language_Processing\" title=\"Template:Natural Language Processing\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Natural_Language_Processing\" title=\"Template talk:Natural Language Processing\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"https://en.wikipedia.org/w/index.php?title=Template:Natural_Language_Processing&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Natural_language_processing\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">General terms</th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Natural_language_understanding\" class=\"mw-redirect\" title=\"Natural language understanding\">Natural language understanding</a></li>\\n<li><a href=\"/wiki/Text_corpus\" title=\"Text corpus\">Text corpus</a></li>\\n<li><a href=\"/wiki/Speech_corpus\" title=\"Speech corpus\">Speech corpus</a></li>\\n<li><a href=\"/wiki/Stop_words\" title=\"Stop words\">Stopwords</a></li>\\n<li><a href=\"/wiki/Bag-of-words_model\" title=\"Bag-of-words model\">Bag-of-words</a></li>\\n<li><a href=\"/wiki/AI-complete\" title=\"AI-complete\">AI-complete</a></li>\\n<li><a href=\"/wiki/N-gram\" title=\"N-gram\">n-gram</a> (<a href=\"/wiki/Bigram\" title=\"Bigram\">Bigram</a>, <a href=\"/wiki/Trigram\" title=\"Trigram\">Trigram</a>)</li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text analysis</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Text_segmentation\" title=\"Text segmentation\">Text segmentation</a></li>\\n<li><a href=\"/wiki/Part-of-speech_tagging\" title=\"Part-of-speech tagging\">Part-of-speech tagging</a></li>\\n<li><a href=\"/wiki/Shallow_parsing\" title=\"Shallow parsing\">Text chunking</a></li>\\n<li><a href=\"/wiki/Compound_term_processing\" class=\"mw-redirect\" title=\"Compound term processing\">Compound term processing</a></li>\\n<li><a href=\"/wiki/Collocation_extraction\" title=\"Collocation extraction\">Collocation extraction</a></li>\\n<li><a href=\"/wiki/Stemming\" title=\"Stemming\">Stemming</a></li>\\n<li><a href=\"/wiki/Lemmatisation\" title=\"Lemmatisation\">Lemmatisation</a></li>\\n<li><a href=\"/wiki/Named-entity_recognition\" title=\"Named-entity recognition\">Named-entity recognition</a></li>\\n<li><a href=\"/wiki/Coreference#Coreference_resolution\" title=\"Coreference\">Coreference resolution</a></li>\\n<li><a href=\"/wiki/Sentiment_analysis\" title=\"Sentiment analysis\">Sentiment analysis</a></li>\\n<li><a href=\"/wiki/Concept_mining\" title=\"Concept mining\">Concept mining</a></li>\\n<li><a href=\"/wiki/Parsing\" title=\"Parsing\">Parsing</a></li>\\n<li><a href=\"/wiki/Word-sense_disambiguation\" title=\"Word-sense disambiguation\">Word-sense disambiguation</a></li>\\n<li><a href=\"/wiki/Ontology_learning\" title=\"Ontology learning\">Ontology learning</a></li>\\n<li><a href=\"/wiki/Terminology_extraction\" title=\"Terminology extraction\">Terminology extraction</a></li>\\n<li><a href=\"/wiki/Textual_entailment\" title=\"Textual entailment\">Textual entailment</a></li>\\n<li><a href=\"/wiki/Truecasing\" title=\"Truecasing\">Truecasing</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Automatic_summarization\" title=\"Automatic summarization\">Automatic summarization</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">Multi-document summarization</a></li>\\n<li><a href=\"/wiki/Sentence_extraction\" title=\"Sentence extraction\">Sentence extraction</a></li>\\n<li><a href=\"/wiki/Text_simplification\" title=\"Text simplification\">Text simplification</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Machine_translation\" title=\"Machine translation\">Machine translation</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Computer-assisted_translation\" title=\"Computer-assisted translation\">Computer-assisted</a></li>\\n<li><a href=\"/wiki/Example-based_machine_translation\" title=\"Example-based machine translation\">Example-based</a></li>\\n<li><a href=\"/wiki/Rule-based_machine_translation\" title=\"Rule-based machine translation\">Rule-based</a></li>\\n<li><a href=\"/wiki/Neural_machine_translation\" title=\"Neural machine translation\">Neural</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Automatic_identification_and_data_capture\" title=\"Automatic identification and data capture\">Automatic identification<br />and data capture</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\\n<li><a href=\"/wiki/Speech_synthesis\" title=\"Speech synthesis\">Speech synthesis</a></li>\\n<li><a class=\"mw-selflink selflink\">Optical character recognition</a></li>\\n<li><a href=\"/wiki/Natural_language_generation\" class=\"mw-redirect\" title=\"Natural language generation\">Natural language generation</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Topic_model\" title=\"Topic model\">Topic model</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Pachinko_allocation\" title=\"Pachinko allocation\">Pachinko allocation</a></li>\\n<li><a href=\"/wiki/Latent_Dirichlet_allocation\" title=\"Latent Dirichlet allocation\">Latent Dirichlet allocation</a></li>\\n<li><a href=\"/wiki/Latent_semantic_analysis\" title=\"Latent semantic analysis\">Latent semantic analysis</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Computer-assisted_reviewing\" title=\"Computer-assisted reviewing\">Computer-assisted<br />reviewing</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Automated_essay_scoring\" title=\"Automated essay scoring\">Automated essay scoring</a></li>\\n<li><a href=\"/wiki/Concordancer\" title=\"Concordancer\">Concordancer</a></li>\\n<li><a href=\"/wiki/Grammar_checker\" title=\"Grammar checker\">Grammar checker</a></li>\\n<li><a href=\"/wiki/Predictive_text\" title=\"Predictive text\">Predictive text</a></li>\\n<li><a href=\"/wiki/Spell_checker\" title=\"Spell checker\">Spell checker</a></li>\\n<li><a href=\"/wiki/Syntax_guessing\" title=\"Syntax guessing\">Syntax guessing</a></li></ul>\\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Natural_language_user_interface\" class=\"mw-redirect\" title=\"Natural language user interface\">Natural language<br />user interface</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\\n<ul><li><a href=\"/wiki/Automated_online_assistant\" class=\"mw-redirect\" title=\"Automated online assistant\">Automated online assistant</a></li>\\n<li><a href=\"/wiki/Chatbot\" title=\"Chatbot\">Chatbot</a></li>\\n<li><a href=\"/wiki/Interactive_fiction\" title=\"Interactive fiction\">Interactive fiction</a></li>\\n<li><a href=\"/wiki/Question_answering\" title=\"Question answering\">Question answering</a></li>\\n<li><a href=\"/wiki/Voice_user_interface\" title=\"Voice user interface\">Voice user interface</a></li></ul>\\n</div></td></tr></tbody></table></div>\\n<!-- \\nNewPP limit report\\nParsed by mw1263\\nCached time: 20190904145017\\nCache expiry: 2592000\\nDynamic content: false\\nComplications: [vary\\xe2\\x80\\x90revision\\xe2\\x80\\x90sha1]\\nCPU time usage: 0.784 seconds\\nReal time usage: 0.969 seconds\\nPreprocessor visited node count: 3464/1000000\\nPreprocessor generated node count: 0/1500000\\nPost\\xe2\\x80\\x90expand include size: 114342/2097152 bytes\\nTemplate argument size: 8099/2097152 bytes\\nHighest expansion depth: 12/40\\nExpensive parser function count: 11/500\\nUnstrip recursion depth: 1/20\\nUnstrip post\\xe2\\x80\\x90expand size: 103321/5000000 bytes\\nNumber of Wikibase entities loaded: 4/400\\nLua time usage: 0.436/10.000 seconds\\nLua memory usage: 6.49 MB/50 MB\\n-->\\n<!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00%  819.629      1 -total\\n 52.85%  433.159      1 Template:Reflist\\n 24.37%  199.705     21 Template:Cite_web\\n 17.79%  145.792      9 Template:Cite_journal\\n 12.97%  106.283      7 Template:Fix\\n 11.00%   90.148      5 Template:Citation_needed\\n  7.23%   59.235      8 Template:Delink\\n  6.25%   51.246     16 Template:Category_handler\\n  4.82%   39.521      1 Template:Commons_category\\n  3.90%   31.969      1 Template:EngvarB\\n-->\\n\\n<!-- Saved in parser cache with key enwiki:pcache:idhash:49091-0!canonical and timestamp 20190904145016 and revision id 912154788\\n -->\\n</div><noscript><img src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" alt=\"\" title=\"\" width=\"1\" height=\"1\" style=\"border: none; position: absolute;\" /></noscript></div>\\n\\t\\t\\n\\t\\t<div class=\"printfooter\">Retrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Optical_character_recognition&amp;oldid=912154788\">https://en.wikipedia.org/w/index.php?title=Optical_character_recognition&amp;oldid=912154788</a>\"</div>\\n\\t\\t\\n\\t\\t<div id=\"catlinks\" class=\"catlinks\" data-mw=\"interface\"><div id=\"mw-normal-catlinks\" class=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Optical_character_recognition\" title=\"Category:Optical character recognition\">Optical character recognition</a></li><li><a href=\"/wiki/Category:Artificial_intelligence_applications\" title=\"Category:Artificial intelligence applications\">Artificial intelligence applications</a></li><li><a href=\"/wiki/Category:Applications_of_computer_vision\" title=\"Category:Applications of computer vision\">Applications of computer vision</a></li><li><a href=\"/wiki/Category:Automatic_identification_and_data_capture\" title=\"Category:Automatic identification and data capture\">Automatic identification and data capture</a></li><li><a href=\"/wiki/Category:Computational_linguistics\" title=\"Category:Computational linguistics\">Computational linguistics</a></li><li><a href=\"/wiki/Category:Unicode\" title=\"Category:Unicode\">Unicode</a></li><li><a href=\"/wiki/Category:Symbols\" title=\"Category:Symbols\">Symbols</a></li><li><a href=\"/wiki/Category:Machine_learning_task\" title=\"Category:Machine learning task\">Machine learning task</a></li></ul></div><div id=\"mw-hidden-catlinks\" class=\"mw-hidden-catlinks mw-hidden-cats-hidden\">Hidden categories: <ul><li><a href=\"/wiki/Category:CS1_errors:_missing_periodical\" title=\"Category:CS1 errors: missing periodical\">CS1 errors: missing periodical</a></li><li><a href=\"/wiki/Category:CS1:_Julian%E2%80%93Gregorian_uncertainty\" title=\"Category:CS1: Julian\\xe2\\x80\\x93Gregorian uncertainty\">CS1: Julian\\xe2\\x80\\x93Gregorian uncertainty</a></li><li><a href=\"/wiki/Category:CS1_errors:_deprecated_parameters\" title=\"Category:CS1 errors: deprecated parameters\">CS1 errors: deprecated parameters</a></li><li><a href=\"/wiki/Category:CS1:_long_volume_value\" title=\"Category:CS1: long volume value\">CS1: long volume value</a></li><li><a href=\"/wiki/Category:CS1_maint:_multiple_names:_authors_list\" title=\"Category:CS1 maint: multiple names: authors list\">CS1 maint: multiple names: authors list</a></li><li><a href=\"/wiki/Category:EngvarB_from_January_2019\" title=\"Category:EngvarB from January 2019\">EngvarB from January 2019</a></li><li><a href=\"/wiki/Category:Articles_with_short_description\" title=\"Category:Articles with short description\">Articles with short description</a></li><li><a href=\"/wiki/Category:Use_mdy_dates_from_January_2019\" title=\"Category:Use mdy dates from January 2019\">Use mdy dates from January 2019</a></li><li><a href=\"/wiki/Category:All_articles_with_unsourced_statements\" title=\"Category:All articles with unsourced statements\">All articles with unsourced statements</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_April_2012\" title=\"Category:Articles with unsourced statements from April 2012\">Articles with unsourced statements from April 2012</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_October_2011\" title=\"Category:Articles with unsourced statements from October 2011\">Articles with unsourced statements from October 2011</a></li><li><a href=\"/wiki/Category:All_articles_with_vague_or_ambiguous_time\" title=\"Category:All articles with vague or ambiguous time\">All articles with vague or ambiguous time</a></li><li><a href=\"/wiki/Category:Vague_or_ambiguous_time_from_March_2013\" title=\"Category:Vague or ambiguous time from March 2013\">Vague or ambiguous time from March 2013</a></li><li><a href=\"/wiki/Category:Wikipedia_articles_needing_clarification_from_March_2013\" title=\"Category:Wikipedia articles needing clarification from March 2013\">Wikipedia articles needing clarification from March 2013</a></li><li><a href=\"/wiki/Category:Wikipedia_articles_in_need_of_updating_from_March_2013\" title=\"Category:Wikipedia articles in need of updating from March 2013\">Wikipedia articles in need of updating from March 2013</a></li><li><a href=\"/wiki/Category:All_Wikipedia_articles_in_need_of_updating\" title=\"Category:All Wikipedia articles in need of updating\">All Wikipedia articles in need of updating</a></li><li><a href=\"/wiki/Category:Articles_with_unsourced_statements_from_May_2009\" title=\"Category:Articles with unsourced statements from May 2009\">Articles with unsourced statements from May 2009</a></li><li><a href=\"/wiki/Category:Commons_category_link_is_on_Wikidata\" title=\"Category:Commons category link is on Wikidata\">Commons category link is on Wikidata</a></li></ul></div></div>\\n\\t\\t<div class=\"visualClear\"></div>\\n\\t\\t\\n\\t</div>\\n</div>\\n<div id=\\'mw-data-after-content\\'>\\n\\t<div class=\"read-more-container\"></div>\\n</div>\\n\\n\\n\\t\\t<div id=\"mw-navigation\">\\n\\t\\t\\t<h2>Navigation menu</h2>\\n\\t\\t\\t<div id=\"mw-head\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-personal\" role=\"navigation\" aria-labelledby=\"p-personal-label\">\\n\\t\\t\\t\\t\\t\\t<h3 id=\"p-personal-label\">Personal tools</h3>\\n\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t\\t\\t<li id=\"pt-anonuserpage\">Not logged in</li><li id=\"pt-anontalk\"><a href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\" accesskey=\"n\">Talk</a></li><li id=\"pt-anoncontribs\"><a href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\" accesskey=\"y\">Contributions</a></li><li id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Optical+character+recognition\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\">Create account</a></li><li id=\"pt-login\"><a href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Optical+character+recognition\" title=\"You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]\" accesskey=\"o\">Log in</a></li>\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"left-navigation\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-namespaces\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-namespaces-label\">\\n\\t\\t\\t\\t\\t\\t<h3 id=\"p-namespaces-label\">Namespaces</h3>\\n\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t\\t\\t<li id=\"ca-nstab-main\" class=\"selected\"><span><a href=\"/wiki/Optical_character_recognition\" title=\"View the content page [c]\" accesskey=\"c\">Article</a></span></li><li id=\"ca-talk\"><span><a href=\"/wiki/Talk:Optical_character_recognition\" rel=\"discussion\" title=\"Discussion about the content page [t]\" accesskey=\"t\">Talk</a></span></li>\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-variants\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-variants-label\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<input type=\"checkbox\" class=\"vectorMenuCheckbox\" aria-labelledby=\"p-variants-label\" />\\n\\t\\t\\t\\t\\t\\t<h3 id=\"p-variants-label\">\\n\\t\\t\\t\\t\\t\\t\\t<span>Variants</span>\\n\\t\\t\\t\\t\\t\\t</h3>\\n\\t\\t\\t\\t\\t\\t<ul class=\"menu\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<div id=\"right-navigation\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-views\" role=\"navigation\" class=\"vectorTabs\" aria-labelledby=\"p-views-label\">\\n\\t\\t\\t\\t\\t\\t<h3 id=\"p-views-label\">Views</h3>\\n\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t\\t\\t<li id=\"ca-view\" class=\"collapsible selected\"><span><a href=\"/wiki/Optical_character_recognition\">Read</a></span></li><li id=\"ca-edit\" class=\"collapsible\"><span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=edit\" title=\"Edit this page [e]\" accesskey=\"e\">Edit</a></span></li><li id=\"ca-history\" class=\"collapsible\"><span><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=history\" title=\"Past revisions of this page [h]\" accesskey=\"h\">View history</a></span></li>\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-cactions\" role=\"navigation\" class=\"vectorMenu emptyPortlet\" aria-labelledby=\"p-cactions-label\">\\n\\t\\t\\t\\t\\t\\t<input type=\"checkbox\" class=\"vectorMenuCheckbox\" aria-labelledby=\"p-cactions-label\" />\\n\\t\\t\\t\\t\\t\\t<h3 id=\"p-cactions-label\"><span>More</span></h3>\\n\\t\\t\\t\\t\\t\\t<ul class=\"menu\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<div id=\"p-search\" role=\"search\">\\n\\t\\t\\t\\t\\t\\t<h3>\\n\\t\\t\\t\\t\\t\\t\\t<label for=\"searchInput\">Search</label>\\n\\t\\t\\t\\t\\t\\t</h3>\\n\\t\\t\\t\\t\\t\\t<form action=\"/w/index.php\" id=\"searchform\">\\n\\t\\t\\t\\t\\t\\t\\t<div id=\"simpleSearch\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<input type=\"search\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" accesskey=\"f\" id=\"searchInput\"/><input type=\"hidden\" value=\"Special:Search\" name=\"title\"/><input type=\"submit\" name=\"fulltext\" value=\"Search\" title=\"Search Wikipedia for this text\" id=\"mw-searchButton\" class=\"searchButton mw-fallbackSearchButton\"/><input type=\"submit\" name=\"go\" value=\"Go\" title=\"Go to a page with this exact name if it exists\" id=\"searchButton\" class=\"searchButton\"/>\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t</form>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t</div>\\n\\t\\t\\t<div id=\"mw-panel\">\\n\\t\\t\\t\\t<div id=\"p-logo\" role=\"banner\"><a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a></div>\\n\\t\\t\\t\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-navigation\" aria-labelledby=\"p-navigation-label\">\\n\\t\\t\\t<h3 id=\"p-navigation-label\">Navigation</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li><li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li><li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content \\xe2\\x80\\x93 the best of Wikipedia\">Featured content</a></li><li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li><li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li><li id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li><li id=\"n-shoplink\"><a href=\"//shop.wikimedia.org\" title=\"Visit the Wikipedia store\">Wikipedia store</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-interaction\" aria-labelledby=\"p-interaction-label\">\\n\\t\\t\\t<h3 id=\"p-interaction-label\">Interaction</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\">Help</a></li><li id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Find out about Wikipedia\">About Wikipedia</a></li><li id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"About the project, what you can do, where to find things\">Community portal</a></li><li id=\"n-recentchanges\"><a href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes in the wiki [r]\" accesskey=\"r\">Recent changes</a></li><li id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\">Contact page</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-tb\" aria-labelledby=\"p-tb-label\">\\n\\t\\t\\t<h3 id=\"p-tb-label\">Tools</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li id=\"t-whatlinkshere\"><a href=\"/wiki/Special:WhatLinksHere/Optical_character_recognition\" title=\"List of all English Wikipedia pages containing links to this page [j]\" accesskey=\"j\">What links here</a></li><li id=\"t-recentchangeslinked\"><a href=\"/wiki/Special:RecentChangesLinked/Optical_character_recognition\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\" accesskey=\"k\">Related changes</a></li><li id=\"t-upload\"><a href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\" accesskey=\"u\">Upload file</a></li><li id=\"t-specialpages\"><a href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\" accesskey=\"q\">Special pages</a></li><li id=\"t-permalink\"><a href=\"/w/index.php?title=Optical_character_recognition&amp;oldid=912154788\" title=\"Permanent link to this revision of the page\">Permanent link</a></li><li id=\"t-info\"><a href=\"/w/index.php?title=Optical_character_recognition&amp;action=info\" title=\"More information about this page\">Page information</a></li><li id=\"t-wikibase\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q167555\" title=\"Link to connected data repository item [g]\" accesskey=\"g\">Wikidata item</a></li><li id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Optical_character_recognition&amp;id=912154788\" title=\"Information on how to cite this page\">Cite this page</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-wikibase-otherprojects\" aria-labelledby=\"p-wikibase-otherprojects-label\">\\n\\t\\t\\t<h3 id=\"p-wikibase-otherprojects-label\">In other projects</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li class=\"wb-otherproject-link wb-otherproject-commons\"><a href=\"https://commons.wikimedia.org/wiki/Category:Optical_character_recognition\" hreflang=\"en\">Wikimedia Commons</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-coll-print_export\" aria-labelledby=\"p-coll-print_export-label\">\\n\\t\\t\\t<h3 id=\"p-coll-print_export-label\">Print/export</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li id=\"coll-create_a_book\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Optical+character+recognition\">Create a book</a></li><li id=\"coll-download-as-rl\"><a href=\"/w/index.php?title=Special:ElectronPdf&amp;page=Optical+character+recognition&amp;action=show-download-screen\">Download as PDF</a></li><li id=\"t-print\"><a href=\"/w/index.php?title=Optical_character_recognition&amp;printable=yes\" title=\"Printable version of this page [p]\" accesskey=\"p\">Printable version</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t<div class=\"portal\" role=\"navigation\" id=\"p-lang\" aria-labelledby=\"p-lang-label\">\\n\\t\\t\\t<h3 id=\"p-lang-label\">Languages</h3>\\n\\t\\t\\t<div class=\"body\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t<li class=\"interlanguage-link interwiki-ar\"><a href=\"https://ar.wikipedia.org/wiki/%D8%AA%D8%B9%D8%B1%D9%81_%D8%B6%D9%88%D8%A6%D9%8A_%D8%B9%D9%84%D9%89_%D8%A7%D9%84%D8%B1%D9%85%D9%88%D8%B2\" title=\"\\xd8\\xaa\\xd8\\xb9\\xd8\\xb1\\xd9\\x81 \\xd8\\xb6\\xd9\\x88\\xd8\\xa6\\xd9\\x8a \\xd8\\xb9\\xd9\\x84\\xd9\\x89 \\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x85\\xd9\\x88\\xd8\\xb2 \\xe2\\x80\\x93 Arabic\" lang=\"ar\" hreflang=\"ar\" class=\"interlanguage-link-target\">\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a\\xd8\\xa9</a></li><li class=\"interlanguage-link interwiki-az\"><a href=\"https://az.wikipedia.org/wiki/Simvollar%C4%B1n_optik_tan%C4%B1nmas%C4%B1\" title=\"Simvollar\\xc4\\xb1n optik tan\\xc4\\xb1nmas\\xc4\\xb1 \\xe2\\x80\\x93 Azerbaijani\" lang=\"az\" hreflang=\"az\" class=\"interlanguage-link-target\">Az\\xc9\\x99rbaycanca</a></li><li class=\"interlanguage-link interwiki-bn\"><a href=\"https://bn.wikipedia.org/wiki/%E0%A6%85%E0%A6%AA%E0%A6%9F%E0%A6%BF%E0%A6%95%E0%A7%8D%E0%A6%AF%E0%A6%BE%E0%A6%B2_%E0%A6%95%E0%A7%8D%E0%A6%AF%E0%A6%BE%E0%A6%B0%E0%A7%87%E0%A6%95%E0%A7%8D%E0%A6%9F%E0%A6%BE%E0%A6%B0_%E0%A6%B0%E0%A6%BF%E0%A6%95%E0%A6%97%E0%A6%A8%E0%A6%BF%E0%A6%B6%E0%A6%A8\" title=\"\\xe0\\xa6\\x85\\xe0\\xa6\\xaa\\xe0\\xa6\\x9f\\xe0\\xa6\\xbf\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xaf\\xe0\\xa6\\xbe\\xe0\\xa6\\xb2 \\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\xaf\\xe0\\xa6\\xbe\\xe0\\xa6\\xb0\\xe0\\xa7\\x87\\xe0\\xa6\\x95\\xe0\\xa7\\x8d\\xe0\\xa6\\x9f\\xe0\\xa6\\xbe\\xe0\\xa6\\xb0 \\xe0\\xa6\\xb0\\xe0\\xa6\\xbf\\xe0\\xa6\\x95\\xe0\\xa6\\x97\\xe0\\xa6\\xa8\\xe0\\xa6\\xbf\\xe0\\xa6\\xb6\\xe0\\xa6\\xa8 \\xe2\\x80\\x93 Bangla\" lang=\"bn\" hreflang=\"bn\" class=\"interlanguage-link-target\">\\xe0\\xa6\\xac\\xe0\\xa6\\xbe\\xe0\\xa6\\x82\\xe0\\xa6\\xb2\\xe0\\xa6\\xbe</a></li><li class=\"interlanguage-link interwiki-bg\"><a href=\"https://bg.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%BD%D0%BE_%D1%80%D0%B0%D0%B7%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B5_%D0%BD%D0%B0_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%B8\" title=\"\\xd0\\x9e\\xd0\\xbf\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd0\\xbe \\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd1\\x81\\xd0\\xb8\\xd0\\xbc\\xd0\\xb2\\xd0\\xbe\\xd0\\xbb\\xd0\\xb8 \\xe2\\x80\\x93 Bulgarian\" lang=\"bg\" hreflang=\"bg\" class=\"interlanguage-link-target\">\\xd0\\x91\\xd1\\x8a\\xd0\\xbb\\xd0\\xb3\\xd0\\xb0\\xd1\\x80\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li><li class=\"interlanguage-link interwiki-bs\"><a href=\"https://bs.wikipedia.org/wiki/Opti%C4%8Dko_prepoznavanje_znakova\" title=\"Opti\\xc4\\x8dko prepoznavanje znakova \\xe2\\x80\\x93 Bosnian\" lang=\"bs\" hreflang=\"bs\" class=\"interlanguage-link-target\">Bosanski</a></li><li class=\"interlanguage-link interwiki-ca\"><a href=\"https://ca.wikipedia.org/wiki/Reconeixement_%C3%B2ptic_de_car%C3%A0cters\" title=\"Reconeixement \\xc3\\xb2ptic de car\\xc3\\xa0cters \\xe2\\x80\\x93 Catalan\" lang=\"ca\" hreflang=\"ca\" class=\"interlanguage-link-target\">Catal\\xc3\\xa0</a></li><li class=\"interlanguage-link interwiki-cs\"><a href=\"https://cs.wikipedia.org/wiki/Optick%C3%A9_rozpozn%C3%A1v%C3%A1n%C3%AD_znak%C5%AF\" title=\"Optick\\xc3\\xa9 rozpozn\\xc3\\xa1v\\xc3\\xa1n\\xc3\\xad znak\\xc5\\xaf \\xe2\\x80\\x93 Czech\" lang=\"cs\" hreflang=\"cs\" class=\"interlanguage-link-target\">\\xc4\\x8ce\\xc5\\xa1tina</a></li><li class=\"interlanguage-link interwiki-da\"><a href=\"https://da.wikipedia.org/wiki/Optical_character_recognition\" title=\"Optical character recognition \\xe2\\x80\\x93 Danish\" lang=\"da\" hreflang=\"da\" class=\"interlanguage-link-target\">Dansk</a></li><li class=\"interlanguage-link interwiki-de\"><a href=\"https://de.wikipedia.org/wiki/Texterkennung\" title=\"Texterkennung \\xe2\\x80\\x93 German\" lang=\"de\" hreflang=\"de\" class=\"interlanguage-link-target\">Deutsch</a></li><li class=\"interlanguage-link interwiki-et\"><a href=\"https://et.wikipedia.org/wiki/Optiline_m%C3%A4rgituvastus\" title=\"Optiline m\\xc3\\xa4rgituvastus \\xe2\\x80\\x93 Estonian\" lang=\"et\" hreflang=\"et\" class=\"interlanguage-link-target\">Eesti</a></li><li class=\"interlanguage-link interwiki-el\"><a href=\"https://el.wikipedia.org/wiki/%CE%9F%CF%80%CF%84%CE%B9%CE%BA%CE%AE_%CE%91%CE%BD%CE%B1%CE%B3%CE%BD%CF%8E%CF%81%CE%B9%CF%83%CE%B7_%CE%A7%CE%B1%CF%81%CE%B1%CE%BA%CF%84%CE%AE%CF%81%CF%89%CE%BD\" title=\"\\xce\\x9f\\xcf\\x80\\xcf\\x84\\xce\\xb9\\xce\\xba\\xce\\xae \\xce\\x91\\xce\\xbd\\xce\\xb1\\xce\\xb3\\xce\\xbd\\xcf\\x8e\\xcf\\x81\\xce\\xb9\\xcf\\x83\\xce\\xb7 \\xce\\xa7\\xce\\xb1\\xcf\\x81\\xce\\xb1\\xce\\xba\\xcf\\x84\\xce\\xae\\xcf\\x81\\xcf\\x89\\xce\\xbd \\xe2\\x80\\x93 Greek\" lang=\"el\" hreflang=\"el\" class=\"interlanguage-link-target\">\\xce\\x95\\xce\\xbb\\xce\\xbb\\xce\\xb7\\xce\\xbd\\xce\\xb9\\xce\\xba\\xce\\xac</a></li><li class=\"interlanguage-link interwiki-es\"><a href=\"https://es.wikipedia.org/wiki/Reconocimiento_%C3%B3ptico_de_caracteres\" title=\"Reconocimiento \\xc3\\xb3ptico de caracteres \\xe2\\x80\\x93 Spanish\" lang=\"es\" hreflang=\"es\" class=\"interlanguage-link-target\">Espa\\xc3\\xb1ol</a></li><li class=\"interlanguage-link interwiki-eo\"><a href=\"https://eo.wikipedia.org/wiki/Optika_signorekono\" title=\"Optika signorekono \\xe2\\x80\\x93 Esperanto\" lang=\"eo\" hreflang=\"eo\" class=\"interlanguage-link-target\">Esperanto</a></li><li class=\"interlanguage-link interwiki-eu\"><a href=\"https://eu.wikipedia.org/wiki/Karaktereen_ezagutze_optiko\" title=\"Karaktereen ezagutze optiko \\xe2\\x80\\x93 Basque\" lang=\"eu\" hreflang=\"eu\" class=\"interlanguage-link-target\">Euskara</a></li><li class=\"interlanguage-link interwiki-fa\"><a href=\"https://fa.wikipedia.org/wiki/%D9%86%D9%88%DB%8C%D8%B3%D9%87%E2%80%8C%D8%AE%D9%88%D8%A7%D9%86_%D9%86%D9%88%D8%B1%DB%8C\" title=\"\\xd9\\x86\\xd9\\x88\\xdb\\x8c\\xd8\\xb3\\xd9\\x87\\xe2\\x80\\x8c\\xd8\\xae\\xd9\\x88\\xd8\\xa7\\xd9\\x86 \\xd9\\x86\\xd9\\x88\\xd8\\xb1\\xdb\\x8c \\xe2\\x80\\x93 Persian\" lang=\"fa\" hreflang=\"fa\" class=\"interlanguage-link-target\">\\xd9\\x81\\xd8\\xa7\\xd8\\xb1\\xd8\\xb3\\xdb\\x8c</a></li><li class=\"interlanguage-link interwiki-fr\"><a href=\"https://fr.wikipedia.org/wiki/Reconnaissance_optique_de_caract%C3%A8res\" title=\"Reconnaissance optique de caract\\xc3\\xa8res \\xe2\\x80\\x93 French\" lang=\"fr\" hreflang=\"fr\" class=\"interlanguage-link-target\">Fran\\xc3\\xa7ais</a></li><li class=\"interlanguage-link interwiki-ga\"><a href=\"https://ga.wikipedia.org/wiki/L%C3%A9itheoir_opt%C3%BAil_carachtar\" title=\"L\\xc3\\xa9itheoir opt\\xc3\\xbail carachtar \\xe2\\x80\\x93 Irish\" lang=\"ga\" hreflang=\"ga\" class=\"interlanguage-link-target\">Gaeilge</a></li><li class=\"interlanguage-link interwiki-gl\"><a href=\"https://gl.wikipedia.org/wiki/Optical_Character_Recognition\" title=\"Optical Character Recognition \\xe2\\x80\\x93 Galician\" lang=\"gl\" hreflang=\"gl\" class=\"interlanguage-link-target\">Galego</a></li><li class=\"interlanguage-link interwiki-ko\"><a href=\"https://ko.wikipedia.org/wiki/%EA%B4%91%ED%95%99_%EB%AC%B8%EC%9E%90_%EC%9D%B8%EC%8B%9D\" title=\"\\xea\\xb4\\x91\\xed\\x95\\x99 \\xeb\\xac\\xb8\\xec\\x9e\\x90 \\xec\\x9d\\xb8\\xec\\x8b\\x9d \\xe2\\x80\\x93 Korean\" lang=\"ko\" hreflang=\"ko\" class=\"interlanguage-link-target\">\\xed\\x95\\x9c\\xea\\xb5\\xad\\xec\\x96\\xb4</a></li><li class=\"interlanguage-link interwiki-hy\"><a href=\"https://hy.wikipedia.org/wiki/%D4%B3%D6%80%D5%A1%D5%B6%D5%B7%D5%A1%D5%B6%D5%B6%D5%A5%D6%80%D5%AB_%D6%85%D5%BA%D5%BF%D5%AB%D5%AF%D5%A1%D5%AF%D5%A1%D5%B6_%D5%B3%D5%A1%D5%B6%D5%A1%D5%B9%D5%B8%D6%82%D5%B4\" title=\"\\xd4\\xb3\\xd6\\x80\\xd5\\xa1\\xd5\\xb6\\xd5\\xb7\\xd5\\xa1\\xd5\\xb6\\xd5\\xb6\\xd5\\xa5\\xd6\\x80\\xd5\\xab \\xd6\\x85\\xd5\\xba\\xd5\\xbf\\xd5\\xab\\xd5\\xaf\\xd5\\xa1\\xd5\\xaf\\xd5\\xa1\\xd5\\xb6 \\xd5\\xb3\\xd5\\xa1\\xd5\\xb6\\xd5\\xa1\\xd5\\xb9\\xd5\\xb8\\xd6\\x82\\xd5\\xb4 \\xe2\\x80\\x93 Armenian\" lang=\"hy\" hreflang=\"hy\" class=\"interlanguage-link-target\">\\xd5\\x80\\xd5\\xa1\\xd5\\xb5\\xd5\\xa5\\xd6\\x80\\xd5\\xa5\\xd5\\xb6</a></li><li class=\"interlanguage-link interwiki-hi\"><a href=\"https://hi.wikipedia.org/wiki/%E0%A4%93%E0%A4%B8%E0%A5%80%E0%A4%86%E0%A4%B0\" title=\"\\xe0\\xa4\\x93\\xe0\\xa4\\xb8\\xe0\\xa5\\x80\\xe0\\xa4\\x86\\xe0\\xa4\\xb0 \\xe2\\x80\\x93 Hindi\" lang=\"hi\" hreflang=\"hi\" class=\"interlanguage-link-target\">\\xe0\\xa4\\xb9\\xe0\\xa4\\xbf\\xe0\\xa4\\xa8\\xe0\\xa5\\x8d\\xe0\\xa4\\xa6\\xe0\\xa5\\x80</a></li><li class=\"interlanguage-link interwiki-hr\"><a href=\"https://hr.wikipedia.org/wiki/Opti%C4%8Dko_prepoznavanje_znakova\" title=\"Opti\\xc4\\x8dko prepoznavanje znakova \\xe2\\x80\\x93 Croatian\" lang=\"hr\" hreflang=\"hr\" class=\"interlanguage-link-target\">Hrvatski</a></li><li class=\"interlanguage-link interwiki-id\"><a href=\"https://id.wikipedia.org/wiki/Pengenalan_karakter_optis\" title=\"Pengenalan karakter optis \\xe2\\x80\\x93 Indonesian\" lang=\"id\" hreflang=\"id\" class=\"interlanguage-link-target\">Bahasa Indonesia</a></li><li class=\"interlanguage-link interwiki-is\"><a href=\"https://is.wikipedia.org/wiki/Lj%C3%B3slestur\" title=\"Lj\\xc3\\xb3slestur \\xe2\\x80\\x93 Icelandic\" lang=\"is\" hreflang=\"is\" class=\"interlanguage-link-target\">\\xc3\\x8dslenska</a></li><li class=\"interlanguage-link interwiki-it\"><a href=\"https://it.wikipedia.org/wiki/Riconoscimento_ottico_dei_caratteri\" title=\"Riconoscimento ottico dei caratteri \\xe2\\x80\\x93 Italian\" lang=\"it\" hreflang=\"it\" class=\"interlanguage-link-target\">Italiano</a></li><li class=\"interlanguage-link interwiki-he\"><a href=\"https://he.wikipedia.org/wiki/%D7%96%D7%99%D7%94%D7%95%D7%99_%D7%AA%D7%95%D7%95%D7%99%D7%9D_%D7%90%D7%95%D7%A4%D7%98%D7%99\" title=\"\\xd7\\x96\\xd7\\x99\\xd7\\x94\\xd7\\x95\\xd7\\x99 \\xd7\\xaa\\xd7\\x95\\xd7\\x95\\xd7\\x99\\xd7\\x9d \\xd7\\x90\\xd7\\x95\\xd7\\xa4\\xd7\\x98\\xd7\\x99 \\xe2\\x80\\x93 Hebrew\" lang=\"he\" hreflang=\"he\" class=\"interlanguage-link-target\">\\xd7\\xa2\\xd7\\x91\\xd7\\xa8\\xd7\\x99\\xd7\\xaa</a></li><li class=\"interlanguage-link interwiki-kk\"><a href=\"https://kk.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%B4%D0%B0%D1%80%D0%B4%D1%8B_%D0%BE%D0%BF%D1%82%D0%B8%D0%BA%D0%B0%D0%BB%D1%8B%D2%9B_%D1%82%D0%B0%D0%BD%D1%8B%D0%BF_%D0%B1%D1%96%D0%BB%D1%83\" title=\"\\xd0\\xa1\\xd0\\xb8\\xd0\\xbc\\xd0\\xb2\\xd0\\xbe\\xd0\\xbb\\xd0\\xb4\\xd0\\xb0\\xd1\\x80\\xd0\\xb4\\xd1\\x8b \\xd0\\xbe\\xd0\\xbf\\xd1\\x82\\xd0\\xb8\\xd0\\xba\\xd0\\xb0\\xd0\\xbb\\xd1\\x8b\\xd2\\x9b \\xd1\\x82\\xd0\\xb0\\xd0\\xbd\\xd1\\x8b\\xd0\\xbf \\xd0\\xb1\\xd1\\x96\\xd0\\xbb\\xd1\\x83 \\xe2\\x80\\x93 Kazakh\" lang=\"kk\" hreflang=\"kk\" class=\"interlanguage-link-target\">\\xd2\\x9a\\xd0\\xb0\\xd0\\xb7\\xd0\\xb0\\xd2\\x9b\\xd1\\x88\\xd0\\xb0</a></li><li class=\"interlanguage-link interwiki-hu\"><a href=\"https://hu.wikipedia.org/wiki/Optikai_karakterfelismer%C3%A9s\" title=\"Optikai karakterfelismer\\xc3\\xa9s \\xe2\\x80\\x93 Hungarian\" lang=\"hu\" hreflang=\"hu\" class=\"interlanguage-link-target\">Magyar</a></li><li class=\"interlanguage-link interwiki-mk\"><a href=\"https://mk.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%BA%D0%BE_%D0%BF%D1%80%D0%B5%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D1%9A%D0%B5_%D0%BD%D0%B0_%D0%B7%D0%BD%D0%B0%D1%86%D0%B8\" title=\"\\xd0\\x9e\\xd0\\xbf\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xba\\xd0\\xbe \\xd0\\xbf\\xd1\\x80\\xd0\\xb5\\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd1\\x9a\\xd0\\xb5 \\xd0\\xbd\\xd0\\xb0 \\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd1\\x86\\xd0\\xb8 \\xe2\\x80\\x93 Macedonian\" lang=\"mk\" hreflang=\"mk\" class=\"interlanguage-link-target\">\\xd0\\x9c\\xd0\\xb0\\xd0\\xba\\xd0\\xb5\\xd0\\xb4\\xd0\\xbe\\xd0\\xbd\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li><li class=\"interlanguage-link interwiki-ml\"><a href=\"https://ml.wikipedia.org/wiki/%E0%B4%92%E0%B4%AA%E0%B5%8D%E0%B4%B1%E0%B5%8D%E0%B4%B1%E0%B4%BF%E0%B4%95%E0%B5%8D%E0%B4%95%E0%B5%BD_%E0%B4%95%E0%B4%BE%E0%B4%B0%E0%B4%95%E0%B5%8D%E0%B4%B1%E0%B5%8D%E0%B4%B1%E0%B5%BC_%E0%B4%B1%E0%B5%86%E0%B4%95%E0%B5%8D%E0%B4%95%E0%B4%97%E0%B5%8D%E0%B4%A8%E0%B4%BF%E0%B4%B7%E0%B5%BB\" title=\"\\xe0\\xb4\\x92\\xe0\\xb4\\xaa\\xe0\\xb5\\x8d\\xe0\\xb4\\xb1\\xe0\\xb5\\x8d\\xe0\\xb4\\xb1\\xe0\\xb4\\xbf\\xe0\\xb4\\x95\\xe0\\xb5\\x8d\\xe0\\xb4\\x95\\xe0\\xb5\\xbd \\xe0\\xb4\\x95\\xe0\\xb4\\xbe\\xe0\\xb4\\xb0\\xe0\\xb4\\x95\\xe0\\xb5\\x8d\\xe0\\xb4\\xb1\\xe0\\xb5\\x8d\\xe0\\xb4\\xb1\\xe0\\xb5\\xbc \\xe0\\xb4\\xb1\\xe0\\xb5\\x86\\xe0\\xb4\\x95\\xe0\\xb5\\x8d\\xe0\\xb4\\x95\\xe0\\xb4\\x97\\xe0\\xb5\\x8d\\xe0\\xb4\\xa8\\xe0\\xb4\\xbf\\xe0\\xb4\\xb7\\xe0\\xb5\\xbb \\xe2\\x80\\x93 Malayalam\" lang=\"ml\" hreflang=\"ml\" class=\"interlanguage-link-target\">\\xe0\\xb4\\xae\\xe0\\xb4\\xb2\\xe0\\xb4\\xaf\\xe0\\xb4\\xbe\\xe0\\xb4\\xb3\\xe0\\xb4\\x82</a></li><li class=\"interlanguage-link interwiki-mr\"><a href=\"https://mr.wikipedia.org/wiki/%E0%A4%B5%E0%A4%BF%E0%A4%95%E0%A4%BF%E0%A4%AA%E0%A5%80%E0%A4%A1%E0%A4%BF%E0%A4%AF%E0%A4%BE:%E0%A4%A7%E0%A5%82%E0%A4%B3%E0%A4%AA%E0%A4%BE%E0%A4%9F%E0%A5%80/%E0%A4%AA%E0%A5%8D%E0%A4%B0%E0%A4%95%E0%A4%BE%E0%A4%B6%E0%A4%95%E0%A5%80%E0%A4%AF_%E0%A4%B6%E0%A4%AC%E0%A5%8D%E0%A4%A6%E0%A4%93%E0%A4%B3%E0%A4%96\" title=\"\\xe0\\xa4\\xb5\\xe0\\xa4\\xbf\\xe0\\xa4\\x95\\xe0\\xa4\\xbf\\xe0\\xa4\\xaa\\xe0\\xa5\\x80\\xe0\\xa4\\xa1\\xe0\\xa4\\xbf\\xe0\\xa4\\xaf\\xe0\\xa4\\xbe:\\xe0\\xa4\\xa7\\xe0\\xa5\\x82\\xe0\\xa4\\xb3\\xe0\\xa4\\xaa\\xe0\\xa4\\xbe\\xe0\\xa4\\x9f\\xe0\\xa5\\x80/\\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa4\\x95\\xe0\\xa4\\xbe\\xe0\\xa4\\xb6\\xe0\\xa4\\x95\\xe0\\xa5\\x80\\xe0\\xa4\\xaf \\xe0\\xa4\\xb6\\xe0\\xa4\\xac\\xe0\\xa5\\x8d\\xe0\\xa4\\xa6\\xe0\\xa4\\x93\\xe0\\xa4\\xb3\\xe0\\xa4\\x96 \\xe2\\x80\\x93 Marathi\" lang=\"mr\" hreflang=\"mr\" class=\"interlanguage-link-target\">\\xe0\\xa4\\xae\\xe0\\xa4\\xb0\\xe0\\xa4\\xbe\\xe0\\xa4\\xa0\\xe0\\xa5\\x80</a></li><li class=\"interlanguage-link interwiki-nl\"><a href=\"https://nl.wikipedia.org/wiki/Optical_character_recognition\" title=\"Optical character recognition \\xe2\\x80\\x93 Dutch\" lang=\"nl\" hreflang=\"nl\" class=\"interlanguage-link-target\">Nederlands</a></li><li class=\"interlanguage-link interwiki-ja\"><a href=\"https://ja.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E6%96%87%E5%AD%97%E8%AA%8D%E8%AD%98\" title=\"\\xe5\\x85\\x89\\xe5\\xad\\xa6\\xe6\\x96\\x87\\xe5\\xad\\x97\\xe8\\xaa\\x8d\\xe8\\xad\\x98 \\xe2\\x80\\x93 Japanese\" lang=\"ja\" hreflang=\"ja\" class=\"interlanguage-link-target\">\\xe6\\x97\\xa5\\xe6\\x9c\\xac\\xe8\\xaa\\x9e</a></li><li class=\"interlanguage-link interwiki-no\"><a href=\"https://no.wikipedia.org/wiki/Optisk_tegngjenkjenning\" title=\"Optisk tegngjenkjenning \\xe2\\x80\\x93 Norwegian\" lang=\"no\" hreflang=\"no\" class=\"interlanguage-link-target\">Norsk</a></li><li class=\"interlanguage-link interwiki-pa\"><a href=\"https://pa.wikipedia.org/wiki/%E0%A8%93%E0%A8%B8%E0%A9%80%E0%A8%86%E0%A8%B0\" title=\"\\xe0\\xa8\\x93\\xe0\\xa8\\xb8\\xe0\\xa9\\x80\\xe0\\xa8\\x86\\xe0\\xa8\\xb0 \\xe2\\x80\\x93 Punjabi\" lang=\"pa\" hreflang=\"pa\" class=\"interlanguage-link-target\">\\xe0\\xa8\\xaa\\xe0\\xa9\\xb0\\xe0\\xa8\\x9c\\xe0\\xa8\\xbe\\xe0\\xa8\\xac\\xe0\\xa9\\x80</a></li><li class=\"interlanguage-link interwiki-pl\"><a href=\"https://pl.wikipedia.org/wiki/Optyczne_rozpoznawanie_znak%C3%B3w\" title=\"Optyczne rozpoznawanie znak\\xc3\\xb3w \\xe2\\x80\\x93 Polish\" lang=\"pl\" hreflang=\"pl\" class=\"interlanguage-link-target\">Polski</a></li><li class=\"interlanguage-link interwiki-pt\"><a href=\"https://pt.wikipedia.org/wiki/Reconhecimento_%C3%B3tico_de_caracteres\" title=\"Reconhecimento \\xc3\\xb3tico de caracteres \\xe2\\x80\\x93 Portuguese\" lang=\"pt\" hreflang=\"pt\" class=\"interlanguage-link-target\">Portugu\\xc3\\xaas</a></li><li class=\"interlanguage-link interwiki-ro\"><a href=\"https://ro.wikipedia.org/wiki/Recunoa%C8%99terea_optic%C4%83_a_caracterelor\" title=\"Recunoa\\xc8\\x99terea optic\\xc4\\x83 a caracterelor \\xe2\\x80\\x93 Romanian\" lang=\"ro\" hreflang=\"ro\" class=\"interlanguage-link-target\">Rom\\xc3\\xa2n\\xc4\\x83</a></li><li class=\"interlanguage-link interwiki-ru\"><a href=\"https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%BE%D0%B2\" title=\"\\xd0\\x9e\\xd0\\xbf\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xb5\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd0\\xb5 \\xd1\\x80\\xd0\\xb0\\xd1\\x81\\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5 \\xd1\\x81\\xd0\\xb8\\xd0\\xbc\\xd0\\xb2\\xd0\\xbe\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2 \\xe2\\x80\\x93 Russian\" lang=\"ru\" hreflang=\"ru\" class=\"interlanguage-link-target\">\\xd0\\xa0\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd0\\xba\\xd0\\xb8\\xd0\\xb9</a></li><li class=\"interlanguage-link interwiki-sco\"><a href=\"https://sco.wikipedia.org/wiki/Optical_chairacter_recogneetion\" title=\"Optical chairacter recogneetion \\xe2\\x80\\x93 Scots\" lang=\"sco\" hreflang=\"sco\" class=\"interlanguage-link-target\">Scots</a></li><li class=\"interlanguage-link interwiki-simple\"><a href=\"https://simple.wikipedia.org/wiki/Optical_character_recognition\" title=\"Optical character recognition \\xe2\\x80\\x93 Simple English\" lang=\"en-simple\" hreflang=\"en-simple\" class=\"interlanguage-link-target\">Simple English</a></li><li class=\"interlanguage-link interwiki-sk\"><a href=\"https://sk.wikipedia.org/wiki/Optick%C3%A9_rozozn%C3%A1vanie_znakov\" title=\"Optick\\xc3\\xa9 rozozn\\xc3\\xa1vanie znakov \\xe2\\x80\\x93 Slovak\" lang=\"sk\" hreflang=\"sk\" class=\"interlanguage-link-target\">Sloven\\xc4\\x8dina</a></li><li class=\"interlanguage-link interwiki-sh\"><a href=\"https://sh.wikipedia.org/wiki/Opti%C4%8Dko_prepoznavanje_znakova\" title=\"Opti\\xc4\\x8dko prepoznavanje znakova \\xe2\\x80\\x93 Serbo-Croatian\" lang=\"sh\" hreflang=\"sh\" class=\"interlanguage-link-target\">Srpskohrvatski / \\xd1\\x81\\xd1\\x80\\xd0\\xbf\\xd1\\x81\\xd0\\xba\\xd0\\xbe\\xd1\\x85\\xd1\\x80\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x81\\xd0\\xba\\xd0\\xb8</a></li><li class=\"interlanguage-link interwiki-fi\"><a href=\"https://fi.wikipedia.org/wiki/Tekstintunnistus\" title=\"Tekstintunnistus \\xe2\\x80\\x93 Finnish\" lang=\"fi\" hreflang=\"fi\" class=\"interlanguage-link-target\">Suomi</a></li><li class=\"interlanguage-link interwiki-sv\"><a href=\"https://sv.wikipedia.org/wiki/Maskinl%C3%A4sning\" title=\"Maskinl\\xc3\\xa4sning \\xe2\\x80\\x93 Swedish\" lang=\"sv\" hreflang=\"sv\" class=\"interlanguage-link-target\">Svenska</a></li><li class=\"interlanguage-link interwiki-ta\"><a href=\"https://ta.wikipedia.org/wiki/%E0%AE%92%E0%AE%B3%E0%AE%BF_%E0%AE%8E%E0%AE%B4%E0%AF%81%E0%AE%A4%E0%AF%8D%E0%AE%A4%E0%AF%81%E0%AE%A3%E0%AE%B0%E0%AE%BF\" title=\"\\xe0\\xae\\x92\\xe0\\xae\\xb3\\xe0\\xae\\xbf \\xe0\\xae\\x8e\\xe0\\xae\\xb4\\xe0\\xaf\\x81\\xe0\\xae\\xa4\\xe0\\xaf\\x8d\\xe0\\xae\\xa4\\xe0\\xaf\\x81\\xe0\\xae\\xa3\\xe0\\xae\\xb0\\xe0\\xae\\xbf \\xe2\\x80\\x93 Tamil\" lang=\"ta\" hreflang=\"ta\" class=\"interlanguage-link-target\">\\xe0\\xae\\xa4\\xe0\\xae\\xae\\xe0\\xae\\xbf\\xe0\\xae\\xb4\\xe0\\xaf\\x8d</a></li><li class=\"interlanguage-link interwiki-te\"><a href=\"https://te.wikipedia.org/wiki/%E0%B0%92%E0%B0%B8%E0%B0%BF%E0%B0%86%E0%B0%B0%E0%B1%8D(OCR)\" title=\"\\xe0\\xb0\\x92\\xe0\\xb0\\xb8\\xe0\\xb0\\xbf\\xe0\\xb0\\x86\\xe0\\xb0\\xb0\\xe0\\xb1\\x8d(OCR) \\xe2\\x80\\x93 Telugu\" lang=\"te\" hreflang=\"te\" class=\"interlanguage-link-target\">\\xe0\\xb0\\xa4\\xe0\\xb1\\x86\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x97\\xe0\\xb1\\x81</a></li><li class=\"interlanguage-link interwiki-th\"><a href=\"https://th.wikipedia.org/wiki/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%88%E0%B8%B3%E0%B8%AD%E0%B8%B1%E0%B8%81%E0%B8%82%E0%B8%A3%E0%B8%B0%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%E0%B9%81%E0%B8%AA%E0%B8%87\" title=\"\\xe0\\xb8\\x81\\xe0\\xb8\\xb2\\xe0\\xb8\\xa3\\xe0\\xb8\\xa3\\xe0\\xb8\\xb9\\xe0\\xb9\\x89\\xe0\\xb8\\x88\\xe0\\xb8\\xb3\\xe0\\xb8\\xad\\xe0\\xb8\\xb1\\xe0\\xb8\\x81\\xe0\\xb8\\x82\\xe0\\xb8\\xa3\\xe0\\xb8\\xb0\\xe0\\xb8\\x94\\xe0\\xb9\\x89\\xe0\\xb8\\xa7\\xe0\\xb8\\xa2\\xe0\\xb9\\x81\\xe0\\xb8\\xaa\\xe0\\xb8\\x87 \\xe2\\x80\\x93 Thai\" lang=\"th\" hreflang=\"th\" class=\"interlanguage-link-target\">\\xe0\\xb9\\x84\\xe0\\xb8\\x97\\xe0\\xb8\\xa2</a></li><li class=\"interlanguage-link interwiki-tr\"><a href=\"https://tr.wikipedia.org/wiki/Optik_karakter_tan%C4%B1ma\" title=\"Optik karakter tan\\xc4\\xb1ma \\xe2\\x80\\x93 Turkish\" lang=\"tr\" hreflang=\"tr\" class=\"interlanguage-link-target\">T\\xc3\\xbcrk\\xc3\\xa7e</a></li><li class=\"interlanguage-link interwiki-uk\"><a href=\"https://uk.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%BD%D0%B5_%D1%80%D0%BE%D0%B7%D0%BF%D1%96%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D1%96%D0%B2\" title=\"\\xd0\\x9e\\xd0\\xbf\\xd1\\x82\\xd0\\xb8\\xd1\\x87\\xd0\\xbd\\xd0\\xb5 \\xd1\\x80\\xd0\\xbe\\xd0\\xb7\\xd0\\xbf\\xd1\\x96\\xd0\\xb7\\xd0\\xbd\\xd0\\xb0\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xbd\\xd1\\x8f \\xd1\\x81\\xd0\\xb8\\xd0\\xbc\\xd0\\xb2\\xd0\\xbe\\xd0\\xbb\\xd1\\x96\\xd0\\xb2 \\xe2\\x80\\x93 Ukrainian\" lang=\"uk\" hreflang=\"uk\" class=\"interlanguage-link-target\">\\xd0\\xa3\\xd0\\xba\\xd1\\x80\\xd0\\xb0\\xd1\\x97\\xd0\\xbd\\xd1\\x81\\xd1\\x8c\\xd0\\xba\\xd0\\xb0</a></li><li class=\"interlanguage-link interwiki-ur\"><a href=\"https://ur.wikipedia.org/wiki/%D8%A8%D8%B5%D8%B1%DB%8C%D8%A7%D8%AA%DB%8C_%D8%AD%D8%B1%D9%81_%D8%B4%D9%86%D8%A7%D8%B3%DB%8C\" title=\"\\xd8\\xa8\\xd8\\xb5\\xd8\\xb1\\xdb\\x8c\\xd8\\xa7\\xd8\\xaa\\xdb\\x8c \\xd8\\xad\\xd8\\xb1\\xd9\\x81 \\xd8\\xb4\\xd9\\x86\\xd8\\xa7\\xd8\\xb3\\xdb\\x8c \\xe2\\x80\\x93 Urdu\" lang=\"ur\" hreflang=\"ur\" class=\"interlanguage-link-target\">\\xd8\\xa7\\xd8\\xb1\\xd8\\xaf\\xd9\\x88</a></li><li class=\"interlanguage-link interwiki-vi\"><a href=\"https://vi.wikipedia.org/wiki/Nh%E1%BA%ADn_d%E1%BA%A1ng_k%C3%BD_t%E1%BB%B1_quang_h%E1%BB%8Dc\" title=\"Nh\\xe1\\xba\\xadn d\\xe1\\xba\\xa1ng k\\xc3\\xbd t\\xe1\\xbb\\xb1 quang h\\xe1\\xbb\\x8dc \\xe2\\x80\\x93 Vietnamese\" lang=\"vi\" hreflang=\"vi\" class=\"interlanguage-link-target\">Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t</a></li><li class=\"interlanguage-link interwiki-yi\"><a href=\"https://yi.wikipedia.org/wiki/%D7%90%D7%A4%D7%98%D7%99%D7%A9%D7%A2_%D7%90%D7%95%D7%AA%D7%99%D7%95%D7%AA_%D7%93%D7%A2%D7%A8%D7%A7%D7%A2%D7%A0%D7%95%D7%A0%D7%92\" title=\"\\xd7\\x90\\xd7\\xa4\\xd7\\x98\\xd7\\x99\\xd7\\xa9\\xd7\\xa2 \\xd7\\x90\\xd7\\x95\\xd7\\xaa\\xd7\\x99\\xd7\\x95\\xd7\\xaa \\xd7\\x93\\xd7\\xa2\\xd7\\xa8\\xd7\\xa7\\xd7\\xa2\\xd7\\xa0\\xd7\\x95\\xd7\\xa0\\xd7\\x92 \\xe2\\x80\\x93 Yiddish\" lang=\"yi\" hreflang=\"yi\" class=\"interlanguage-link-target\">\\xd7\\x99\\xd7\\x99\\xd6\\xb4\\xd7\\x93\\xd7\\x99\\xd7\\xa9</a></li><li class=\"interlanguage-link interwiki-zh\"><a href=\"https://zh.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB\" title=\"\\xe5\\x85\\x89\\xe5\\xad\\xa6\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe8\\xaf\\x86\\xe5\\x88\\xab \\xe2\\x80\\x93 Chinese\" lang=\"zh\" hreflang=\"zh\" class=\"interlanguage-link-target\">\\xe4\\xb8\\xad\\xe6\\x96\\x87</a></li>\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-edit wb-langlinks-link\"><a href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q167555#sitelinks-wikipedia\" title=\"Edit interlanguage links\" class=\"wbc-editpage\">Edit links</a></span></div>\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t</div>\\n\\t\\t\\t\\t<div id=\"footer\" role=\"contentinfo\">\\n\\t\\t\\t\\t\\t\\t<ul id=\"footer-info\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-info-lastmod\"> This page was last edited on 23 August 2019, at 16:20<span class=\"anonymous-show\">&#160;(UTC)</span>.</li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-info-copyright\">Text is available under the <a rel=\"license\" href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\">Creative Commons Attribution-ShareAlike License</a><a rel=\"license\" href=\"//creativecommons.org/licenses/by-sa/3.0/\" style=\"display:none;\"></a>;\\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia\\xc2\\xae is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\\n\\t\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t<ul id=\"footer-places\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-privacy\"><a href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\" class=\"extiw\" title=\"wmf:Privacy policy\">Privacy policy</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\\n\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-places-mobileview\"><a href=\"//en.m.wikipedia.org/w/index.php?title=Optical_character_recognition&amp;mobileaction=toggle_view_mobile\" class=\"noprint stopMobileRedirectToggle\">Mobile view</a></li>\\n\\t\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<ul id=\"footer-icons\" class=\"noprint\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-copyrightico\">\\n\\t\\t\\t\\t\\t\\t<a href=\"https://wikimediafoundation.org/\"><img src=\"/static/images/wikimedia-button.png\" srcset=\"/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x\" width=\"88\" height=\"31\" alt=\"Wikimedia Foundation\"/></a>\\t\\t\\t\\t\\t</li>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<li id=\"footer-poweredbyico\">\\n\\t\\t\\t\\t\\t\\t<a href=\"https://www.mediawiki.org/\"><img src=\"/static/images/poweredby_mediawiki_88x31.png\" alt=\"Powered by MediaWiki\" srcset=\"/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x\" width=\"88\" height=\"31\"/></a>\\t\\t\\t\\t\\t</li>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t\\t\\t<div style=\"clear: both;\"></div>\\n\\t\\t</div>\\n\\t\\t\\n\\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.784\",\"walltime\":\"0.969\",\"ppvisitednodes\":{\"value\":3464,\"limit\":1000000},\"ppgeneratednodes\":{\"value\":0,\"limit\":1500000},\"postexpandincludesize\":{\"value\":114342,\"limit\":2097152},\"templateargumentsize\":{\"value\":8099,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":40},\"expensivefunctioncount\":{\"value\":11,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":103321,\"limit\":5000000},\"entityaccesscount\":{\"value\":4,\"limit\":400},\"timingprofile\":[\"100.00%  819.629      1 -total\",\" 52.85%  433.159      1 Template:Reflist\",\" 24.37%  199.705     21 Template:Cite_web\",\" 17.79%  145.792      9 Template:Cite_journal\",\" 12.97%  106.283      7 Template:Fix\",\" 11.00%   90.148      5 Template:Citation_needed\",\"  7.23%   59.235      8 Template:Delink\",\"  6.25%   51.246     16 Template:Category_handler\",\"  4.82%   39.521      1 Template:Commons_category\",\"  3.90%   31.969      1 Template:EngvarB\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.436\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":6801050,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw1263\",\"timestamp\":\"20190904145017\",\"ttl\":2592000,\"transientcontent\":false}}});});</script>\\n<script type=\"application/ld+json\">{\"@context\":\"https:\\\\/\\\\/schema.org\",\"@type\":\"Article\",\"name\":\"Optical character recognition\",\"url\":\"https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Optical_character_recognition\",\"sameAs\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q167555\",\"mainEntity\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q167555\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\/\\\\/www.wikimedia.org\\\\/static\\\\/images\\\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2002-04-14T15:48:48Z\",\"dateModified\":\"2019-08-23T16:20:33Z\",\"image\":\"https:\\\\/\\\\/upload.wikimedia.org\\\\/wikipedia\\\\/commons\\\\/8\\\\/81\\\\/Portable_scanner_and_OCR_%28video%29.webm\",\"headline\":\"computer recognition of visual text\"}</script>\\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgBackendResponseTime\":141,\"wgHostname\":\"mw1264\"});});</script>\\n</body>\\n</html>\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcrKvOdCOlFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = bs.BeautifulSoup(source,'lxml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I19oWLyuO2_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# soup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k_3n4NxO355",
        "colab_type": "code",
        "outputId": "83c8caed-dbb9-41e0-9963-3bdea3ad5bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text = \"\"\n",
        "for paragraph in soup.find_all('p'):\n",
        "  text += paragraph.text\n",
        "  \n",
        "text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOptical character recognition or optical character reader (OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast).[1]\\nWidely used as a form of information entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.\\nEarly versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs.[2] Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.\\nEarly optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind.[3] In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code.[citation needed] Concurrently, Edmund Fournier d\\'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.[4]\\nIn the late 1920s and into the 1930s Emanuel Goldberg developed what he called a \"Statistical Machine\" for searching microfilm archives using an optical code recognition system. In 1931 he was granted USA Patent number 1,838,389 for the invention. The patent was acquired by IBM.\\n\\nIn 1974, Ray Kurzweil started the company Kurzweil Computer Products, Inc. and continued development of omni-font OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s[3][5]). Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud. This device required the invention of two enabling technologies\\xa0– the CCD flatbed scanner and the text-to-speech synthesizer. On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the National Federation of the Blind.[citation needed] In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. LexisNexis was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases. Two years later, Kurzweil sold his company to Xerox, which had an interest in further commercializing paper-to-computer text conversion. Xerox eventually spun it off as Scansoft, which merged with Nuance Communications.[citation needed] The research group headed by A. G. Ramakrishnan at the Medical intelligence and language engineering lab, Indian Institute of Science, has developed PrintToBraille tool, an open source GUI front end[6] that can be used by any OCR to convert scanned images of printed books to Braille books.\\nIn the 2000s, OCR was made available online as a service (WebOCR), in a cloud computing environment, and in mobile applications like real-time translation of foreign-language signs on a smartphone. With the advent of smart-phones and smartglasses, OCR can be used in internet connected mobile device applications that extract text captured using the device\\'s camera. These devices that do not have OCR functionality built into the operating system will typically use an OCR API to extract the text from the image file captured and provided by the device.[7][8] The OCR API returns the extracted text, along with information about the location of the detected text in the original image back to the device app for further processing (such as text-to-speech) or display.\\nVarious commercial and open source OCR systems are available for most common writing systems, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters.\\nOCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.\\nThey can be used for:\\nOCR is generally an \"offline\" process, which analyses a static document. Handwriting movement analysis can be used as input to handwriting recognition.[13]  Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which segments are drawn, the direction, and the pattern of putting the pen down and lifting it.  This additional information can make the end-to-end process more accurate.  This technology is also known as \"on-line character recognition\", \"dynamic character recognition\", \"real-time character recognition\", and \"intelligent character recognition\".\\nOCR software often \"pre-processes\" images to improve the chances of successful recognition. Techniques include:[14]\\nSegmentation of fixed-pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas.  For proportional fonts, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character.[21]\\nThere are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters.[22]\\nMatrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as \"pattern matching\", \"pattern recognition\", or \"image correlation\". This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale.  This technique works best with typewritten text and does not work well when new fonts are encountered.  This is the technique the early physical photocell-based OCR implemented, rather directly.\\nFeature extraction decomposes glyphs into \"features\" like lines, closed loops, line direction, and line intersections.  The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient. These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes.  General techniques of feature detection in computer vision are applicable to this type of OCR, which is commonly seen in \"intelligent\" handwriting recognition and indeed most modern OCR software.[23] Nearest neighbour classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match.[24]\\nSoftware such as Cuneiform and Tesseract use a two-pass approach to character recognition.  The second pass is known as \"adaptive recognition\" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass.  This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g. blurred or faded).[21]\\nThe OCR result can be stored in the standardized ALTO format, a dedicated XML schema maintained by the United States Library of Congress.\\nFor a list of optical character recognition software see Comparison of optical character recognition software.\\nOCR accuracy can be increased if the output is constrained by a lexicon\\xa0– a list of words that are allowed to occur in a document.[14]  This might be, for example, all the words in the English language, or a more technical lexicon for a specific field.  This technique can be problematic if the document contains words not in the lexicon, like proper nouns. Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy.[21]\\nThe output stream may be a plain text stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated PDF that includes both the original image of the page and a searchable textual representation.\\n\"Near-neighbor analysis\" can make use of co-occurrence frequencies to correct errors, by noting that certain words are often seen together.[25]  For example, \"Washington, D.C.\" is generally far more common in English than \"Washington DOC\".\\nKnowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy.\\nThe Levenshtein Distance algorithm has also been used in OCR post-processing to further optimize results from an OCR API.[26]\\nIn recent years,[when?] the major OCR technology providers began to tweak OCR systems to deal more efficiently with specific types of input. Beyond an application-specific lexicon, better performance may be had by taking into account business rules, standard expression,[clarification needed] or rich information contained in color images. This strategy is called \"Application-Oriented OCR\" or \"Customized OCR\", and has been applied to OCR of license plates, invoices, screenshots, ID cards, driver licenses, and automobile manufacturing.\\nThe New York Times has adapted the OCR technology into a proprietary tool they entitle, Document Helper, that enables their interactive news team to accelerate the processing of documents that need to be reviewed. They note that it enables them to process what amounts to as many as 5,400 pages per hour in preparation for reporters to review the contents.[27]\\nThere are several techniques for solving the problem of character recognition by means other than improved OCR algorithms.\\nSpecial fonts like OCR-A, OCR-B, or MICR fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription in bank check processing. Ironically however, several prominent OCR engines were designed to capture text in popular fonts such as Arial or Times New Roman, and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts. As Google Tesseract can be trained to recognize new fonts, it can recognize OCR-A, OCR-B and MICR fonts[28].\\n\"Comb fields\" are pre-printed boxes that encourage humans to write more legibly\\xa0– one glyph per box.[25]  These are often printed in a \"dropout color\" which can be easily removed by the OCR system.[25]\\nPalm OS used a special set of glyphs, known as \"Graffiti\" which are similar to printed English characters but simplified or modified for easier recognition on the platform\\'s computationally limited hardware. Users would need to learn how to write these special glyphs.\\nZone-based OCR restricts the image to a specific part of a document.  This is often referred to as \"Template OCR\".\\nCrowdsourcing humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than is obtained with computers.  Practical systems include the Amazon Mechanical Turk and reCAPTCHA. The National Library of Finland has developed an online interface for users to correct OCRed texts in the standardized ALTO format.[29] Crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms, for example, through the use of rank-order tournaments.[30]\\nCommissioned by the U.S. Department of Energy (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the Annual Test of OCR Accuracy from 1992 to 1996.[31]\\nRecognition of Latin-script, typewritten text is still not 100% accurate even where clear imaging is available. One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%;[32] total accuracy can be achieved by human review or Data Dictionary Authentication. Other areas—including recognition of hand printing, cursive handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)—are still the subject of active research. The MNIST database is commonly used for testing systems\\' ability to recognise handwritten digits.\\nAccuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate. For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters.[33]\\nAn example of the difficulties inherent in digitizing old text is the inability of OCR to differentiate between the \"long s\" and \"f\" characters.[34]\\nWeb-based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years[when?] (see Tablet PC history). Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by pen computing software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.[citation needed]\\nRecognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text. Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information. For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script. Reading the Amount line of a cheque (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly. The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.[citation needed]\\nMost programs allow users to set \"confidence rates\". This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review.\\nCharacters to support OCR were added to the Unicode Standard in June 1993, with the release of version 1.1.\\nSome of these characters are mapped from fonts specific to MICR, OCR-A or OCR-B.\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgGgnn-YX14w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo6M2dFmPsEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTKHLidKQFYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = re.sub(r\"\\[[0-9]*\\]\",\" \",text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLBMGvypQ5wJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = re.sub(r'\\s+',' ',text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQJ4J3ZXQ6wo",
        "colab_type": "code",
        "outputId": "956b7741-78c6-4c65-d880-a94034977e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Optical character recognition or optical character reader (OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast). Widely used as a form of information entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision. Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components. Early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind. In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code.[citation needed] Concurrently, Edmund Fournier d\\'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters. In the late 1920s and into the 1930s Emanuel Goldberg developed what he called a \"Statistical Machine\" for searching microfilm archives using an optical code recognition system. In 1931 he was granted USA Patent number 1,838,389 for the invention. The patent was acquired by IBM. In 1974, Ray Kurzweil started the company Kurzweil Computer Products, Inc. and continued development of omni-font OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s ). Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud. This device required the invention of two enabling technologies – the CCD flatbed scanner and the text-to-speech synthesizer. On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the National Federation of the Blind.[citation needed] In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program. LexisNexis was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases. Two years later, Kurzweil sold his company to Xerox, which had an interest in further commercializing paper-to-computer text conversion. Xerox eventually spun it off as Scansoft, which merged with Nuance Communications.[citation needed] The research group headed by A. G. Ramakrishnan at the Medical intelligence and language engineering lab, Indian Institute of Science, has developed PrintToBraille tool, an open source GUI front end that can be used by any OCR to convert scanned images of printed books to Braille books. In the 2000s, OCR was made available online as a service (WebOCR), in a cloud computing environment, and in mobile applications like real-time translation of foreign-language signs on a smartphone. With the advent of smart-phones and smartglasses, OCR can be used in internet connected mobile device applications that extract text captured using the device\\'s camera. These devices that do not have OCR functionality built into the operating system will typically use an OCR API to extract the text from the image file captured and provided by the device. The OCR API returns the extracted text, along with information about the location of the detected text in the original image back to the device app for further processing (such as text-to-speech) or display. Various commercial and open source OCR systems are available for most common writing systems, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters. OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR. They can be used for: OCR is generally an \"offline\" process, which analyses a static document. Handwriting movement analysis can be used as input to handwriting recognition. Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which segments are drawn, the direction, and the pattern of putting the pen down and lifting it. This additional information can make the end-to-end process more accurate. This technology is also known as \"on-line character recognition\", \"dynamic character recognition\", \"real-time character recognition\", and \"intelligent character recognition\". OCR software often \"pre-processes\" images to improve the chances of successful recognition. Techniques include: Segmentation of fixed-pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas. For proportional fonts, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character. There are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters. Matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as \"pattern matching\", \"pattern recognition\", or \"image correlation\". This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale. This technique works best with typewritten text and does not work well when new fonts are encountered. This is the technique the early physical photocell-based OCR implemented, rather directly. Feature extraction decomposes glyphs into \"features\" like lines, closed loops, line direction, and line intersections. The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient. These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes. General techniques of feature detection in computer vision are applicable to this type of OCR, which is commonly seen in \"intelligent\" handwriting recognition and indeed most modern OCR software. Nearest neighbour classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match. Software such as Cuneiform and Tesseract use a two-pass approach to character recognition. The second pass is known as \"adaptive recognition\" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass. This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g. blurred or faded). The OCR result can be stored in the standardized ALTO format, a dedicated XML schema maintained by the United States Library of Congress. For a list of optical character recognition software see Comparison of optical character recognition software. OCR accuracy can be increased if the output is constrained by a lexicon – a list of words that are allowed to occur in a document. This might be, for example, all the words in the English language, or a more technical lexicon for a specific field. This technique can be problematic if the document contains words not in the lexicon, like proper nouns. Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy. The output stream may be a plain text stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated PDF that includes both the original image of the page and a searchable textual representation. \"Near-neighbor analysis\" can make use of co-occurrence frequencies to correct errors, by noting that certain words are often seen together. For example, \"Washington, D.C.\" is generally far more common in English than \"Washington DOC\". Knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy. The Levenshtein Distance algorithm has also been used in OCR post-processing to further optimize results from an OCR API. In recent years,[when?] the major OCR technology providers began to tweak OCR systems to deal more efficiently with specific types of input. Beyond an application-specific lexicon, better performance may be had by taking into account business rules, standard expression,[clarification needed] or rich information contained in color images. This strategy is called \"Application-Oriented OCR\" or \"Customized OCR\", and has been applied to OCR of license plates, invoices, screenshots, ID cards, driver licenses, and automobile manufacturing. The New York Times has adapted the OCR technology into a proprietary tool they entitle, Document Helper, that enables their interactive news team to accelerate the processing of documents that need to be reviewed. They note that it enables them to process what amounts to as many as 5,400 pages per hour in preparation for reporters to review the contents. There are several techniques for solving the problem of character recognition by means other than improved OCR algorithms. Special fonts like OCR-A, OCR-B, or MICR fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription in bank check processing. Ironically however, several prominent OCR engines were designed to capture text in popular fonts such as Arial or Times New Roman, and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts. As Google Tesseract can be trained to recognize new fonts, it can recognize OCR-A, OCR-B and MICR fonts . \"Comb fields\" are pre-printed boxes that encourage humans to write more legibly – one glyph per box. These are often printed in a \"dropout color\" which can be easily removed by the OCR system. Palm OS used a special set of glyphs, known as \"Graffiti\" which are similar to printed English characters but simplified or modified for easier recognition on the platform\\'s computationally limited hardware. Users would need to learn how to write these special glyphs. Zone-based OCR restricts the image to a specific part of a document. This is often referred to as \"Template OCR\". Crowdsourcing humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than is obtained with computers. Practical systems include the Amazon Mechanical Turk and reCAPTCHA. The National Library of Finland has developed an online interface for users to correct OCRed texts in the standardized ALTO format. Crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms, for example, through the use of rank-order tournaments. Commissioned by the U.S. Department of Energy (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the Annual Test of OCR Accuracy from 1992 to 1996. Recognition of Latin-script, typewritten text is still not 100% accurate even where clear imaging is available. One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%; total accuracy can be achieved by human review or Data Dictionary Authentication. Other areas—including recognition of hand printing, cursive handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)—are still the subject of active research. The MNIST database is commonly used for testing systems\\' ability to recognise handwritten digits. Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate. For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters. An example of the difficulties inherent in digitizing old text is the inability of OCR to differentiate between the \"long s\" and \"f\" characters. Web-based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years[when?] (see Tablet PC history). Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by pen computing software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.[citation needed] Recognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text. Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information. For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script. Reading the Amount line of a cheque (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly. The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.[citation needed] Most programs allow users to set \"confidence rates\". This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review. Characters to support OCR were added to the Unicode Standard in June 1993, with the release of version 1.1. Some of these characters are mapped from fonts specific to MICR, OCR-A or OCR-B. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7hM750Rqhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-FAaCizSH8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text = text.lower()\n",
        "clean_text = re.sub(r'\\W',\" \",clean_text)\n",
        "clean_text = re.sub(r'\\d',\" \",clean_text)\n",
        "clean_text = re.sub(r'\\s+',\" \",clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph2YGm3XSnr_",
        "colab_type": "code",
        "outputId": "d3ee31fc-a264-4d1e-b3d6-255b75fa3333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "clean_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' optical character recognition or optical character reader ocr is the mechanical or electronic conversion of images of typed handwritten or printed text into machine encoded text whether from a scanned document a photo of a document a scene photo for example the text on signs and billboards in a landscape photo or from subtitle text superimposed on an image for example from a television broadcast widely used as a form of information entry from printed paper data records whether passport documents invoices bank statements computerized receipts business cards mail printouts of static data or any suitable documentation it is a common method of digitizing printed texts so that they can be electronically edited searched stored more compactly displayed on line and used in machine processes such as cognitive computing machine translation extracted text to speech key data and text mining ocr is a field of research in pattern recognition artificial intelligence and computer vision early versions needed to be trained with images of each character and worked on one font at a time advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common and with support for a variety of digital image file format inputs some systems are capable of reproducing formatted output that closely approximates the original page including images columns and other non textual components early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind in emanuel goldberg developed a machine that read characters and converted them into standard telegraph code citation needed concurrently edmund fournier d albe developed the optophone a handheld scanner that when moved across a printed page produced tones that corresponded to specific letters or characters in the late s and into the s emanuel goldberg developed what he called a statistical machine for searching microfilm archives using an optical code recognition system in he was granted usa patent number for the invention the patent was acquired by ibm in ray kurzweil started the company kurzweil computer products inc and continued development of omni font ocr which could recognize text printed in virtually any font kurzweil is often credited with inventing omni font ocr but it was in use by companies including compuscan in the late s and s kurzweil decided that the best application of this technology would be to create a reading machine for the blind which would allow blind people to have a computer read text to them out loud this device required the invention of two enabling technologies the ccd flatbed scanner and the text to speech synthesizer on january the successful finished product was unveiled during a widely reported news conference headed by kurzweil and the leaders of the national federation of the blind citation needed in kurzweil computer products began selling a commercial version of the optical character recognition computer program lexisnexis was one of the first customers and bought the program to upload legal paper and news documents onto its nascent online databases two years later kurzweil sold his company to xerox which had an interest in further commercializing paper to computer text conversion xerox eventually spun it off as scansoft which merged with nuance communications citation needed the research group headed by a g ramakrishnan at the medical intelligence and language engineering lab indian institute of science has developed printtobraille tool an open source gui front end that can be used by any ocr to convert scanned images of printed books to braille books in the s ocr was made available online as a service webocr in a cloud computing environment and in mobile applications like real time translation of foreign language signs on a smartphone with the advent of smart phones and smartglasses ocr can be used in internet connected mobile device applications that extract text captured using the device s camera these devices that do not have ocr functionality built into the operating system will typically use an ocr api to extract the text from the image file captured and provided by the device the ocr api returns the extracted text along with information about the location of the detected text in the original image back to the device app for further processing such as text to speech or display various commercial and open source ocr systems are available for most common writing systems including latin cyrillic arabic hebrew indic bengali bangla devanagari tamil chinese japanese and korean characters ocr engines have been developed into many kinds of domain specific ocr applications such as receipt ocr invoice ocr check ocr legal billing document ocr they can be used for ocr is generally an offline process which analyses a static document handwriting movement analysis can be used as input to handwriting recognition instead of merely using the shapes of glyphs and words this technique is able to capture motions such as the order in which segments are drawn the direction and the pattern of putting the pen down and lifting it this additional information can make the end to end process more accurate this technology is also known as on line character recognition dynamic character recognition real time character recognition and intelligent character recognition ocr software often pre processes images to improve the chances of successful recognition techniques include segmentation of fixed pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas for proportional fonts more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words and vertical lines can intersect more than one character there are two basic types of core ocr algorithm which may produce a ranked list of candidate characters matrix matching involves comparing an image to a stored glyph on a pixel by pixel basis it is also known as pattern matching pattern recognition or image correlation this relies on the input glyph being correctly isolated from the rest of the image and on the stored glyph being in a similar font and at the same scale this technique works best with typewritten text and does not work well when new fonts are encountered this is the technique the early physical photocell based ocr implemented rather directly feature extraction decomposes glyphs into features like lines closed loops line direction and line intersections the extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient these features are compared with an abstract vector like representation of a character which might reduce to one or more glyph prototypes general techniques of feature detection in computer vision are applicable to this type of ocr which is commonly seen in intelligent handwriting recognition and indeed most modern ocr software nearest neighbour classifiers such as the k nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match software such as cuneiform and tesseract use a two pass approach to character recognition the second pass is known as adaptive recognition and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass this is advantageous for unusual fonts or low quality scans where the font is distorted e g blurred or faded the ocr result can be stored in the standardized alto format a dedicated xml schema maintained by the united states library of congress for a list of optical character recognition software see comparison of optical character recognition software ocr accuracy can be increased if the output is constrained by a lexicon a list of words that are allowed to occur in a document this might be for example all the words in the english language or a more technical lexicon for a specific field this technique can be problematic if the document contains words not in the lexicon like proper nouns tesseract uses its dictionary to influence the character segmentation step for improved accuracy the output stream may be a plain text stream or file of characters but more sophisticated ocr systems can preserve the original layout of the page and produce for example an annotated pdf that includes both the original image of the page and a searchable textual representation near neighbor analysis can make use of co occurrence frequencies to correct errors by noting that certain words are often seen together for example washington d c is generally far more common in english than washington doc knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun for example allowing greater accuracy the levenshtein distance algorithm has also been used in ocr post processing to further optimize results from an ocr api in recent years when the major ocr technology providers began to tweak ocr systems to deal more efficiently with specific types of input beyond an application specific lexicon better performance may be had by taking into account business rules standard expression clarification needed or rich information contained in color images this strategy is called application oriented ocr or customized ocr and has been applied to ocr of license plates invoices screenshots id cards driver licenses and automobile manufacturing the new york times has adapted the ocr technology into a proprietary tool they entitle document helper that enables their interactive news team to accelerate the processing of documents that need to be reviewed they note that it enables them to process what amounts to as many as pages per hour in preparation for reporters to review the contents there are several techniques for solving the problem of character recognition by means other than improved ocr algorithms special fonts like ocr a ocr b or micr fonts with precisely specified sizing spacing and distinctive character shapes allow a higher accuracy rate during transcription in bank check processing ironically however several prominent ocr engines were designed to capture text in popular fonts such as arial or times new roman and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts as google tesseract can be trained to recognize new fonts it can recognize ocr a ocr b and micr fonts comb fields are pre printed boxes that encourage humans to write more legibly one glyph per box these are often printed in a dropout color which can be easily removed by the ocr system palm os used a special set of glyphs known as graffiti which are similar to printed english characters but simplified or modified for easier recognition on the platform s computationally limited hardware users would need to learn how to write these special glyphs zone based ocr restricts the image to a specific part of a document this is often referred to as template ocr crowdsourcing humans to perform the character recognition can quickly process images like computer driven ocr but with higher accuracy for recognizing images than is obtained with computers practical systems include the amazon mechanical turk and recaptcha the national library of finland has developed an online interface for users to correct ocred texts in the standardized alto format crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms for example through the use of rank order tournaments commissioned by the u s department of energy doe the information science research institute isri had the mission to foster the improvement of automated technologies for understanding machine printed documents and it conducted the most authoritative of the annual test of ocr accuracy from to recognition of latin script typewritten text is still not accurate even where clear imaging is available one study based on recognition of th and early th century newspaper pages concluded that character by character ocr accuracy for commercial ocr software varied from to total accuracy can be achieved by human review or data dictionary authentication other areas including recognition of hand printing cursive handwriting and printed text in other scripts especially those east asian language characters which have many strokes for a single character are still the subject of active research the mnist database is commonly used for testing systems ability to recognise handwritten digits accuracy rates can be measured in several ways and how they are measured can greatly affect the reported accuracy rate for example if word context basically a lexicon of words is not used to correct software finding non existent words a character error rate of accuracy may result in an error rate of accuracy or worse if the measurement is based on whether each whole word was recognized with no incorrect letters an example of the difficulties inherent in digitizing old text is the inability of ocr to differentiate between the long s and f characters web based ocr systems for recognizing hand printed text on the fly have become well known as commercial products in recent years when see tablet pc history accuracy rates of to on neat clean hand printed characters can be achieved by pen computing software but that accuracy rate still translates to dozens of errors per page making the technology useful only in very limited applications citation needed recognition of cursive text is an active area of research with recognition rates even lower than that of hand printed text higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information for example recognizing entire words from a dictionary is easier than trying to parse individual characters from script reading the amount line of a cheque which is always a written out number is an example where using a smaller dictionary can increase recognition rates greatly the shapes of individual cursive characters themselves simply do not contain enough information to accurately greater than recognize all handwritten cursive script citation needed most programs allow users to set confidence rates this means that if the software does not achieve their desired level of accuracy a user can be notified for manual review characters to support ocr were added to the unicode standard in june with the release of version some of these characters are mapped from fonts specific to micr ocr a or ocr b '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJqF1jgNUd3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6DZl4EKVpBk",
        "colab_type": "code",
        "outputId": "5ee8fbb4-d845-454b-bc6c-0b86b40cb593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8QU0xBoSp2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = nltk.sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4CnY-GzVQ-g",
        "colab_type": "code",
        "outputId": "8accff70-f497-46ae-9731-a09c4ce792d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for sent in sentences:\n",
        "  print(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Optical character recognition or optical character reader (OCR) is the mechanical or electronic conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example from a television broadcast).\n",
            "Widely used as a form of information entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining.\n",
            "OCR is a field of research in pattern recognition, artificial intelligence and computer vision.\n",
            "Early versions needed to be trained with images of each character, and worked on one font at a time.\n",
            "Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs.\n",
            "Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.\n",
            "Early optical character recognition may be traced to technologies involving telegraphy and creating reading devices for the blind.\n",
            "In 1914, Emanuel Goldberg developed a machine that read characters and converted them into standard telegraph code.\n",
            "[citation needed] Concurrently, Edmund Fournier d'Albe developed the Optophone, a handheld scanner that when moved across a printed page, produced tones that corresponded to specific letters or characters.\n",
            "In the late 1920s and into the 1930s Emanuel Goldberg developed what he called a \"Statistical Machine\" for searching microfilm archives using an optical code recognition system.\n",
            "In 1931 he was granted USA Patent number 1,838,389 for the invention.\n",
            "The patent was acquired by IBM.\n",
            "In 1974, Ray Kurzweil started the company Kurzweil Computer Products, Inc. and continued development of omni-font OCR, which could recognize text printed in virtually any font (Kurzweil is often credited with inventing omni-font OCR, but it was in use by companies, including CompuScan, in the late 1960s and 1970s ).\n",
            "Kurzweil decided that the best application of this technology would be to create a reading machine for the blind, which would allow blind people to have a computer read text to them out loud.\n",
            "This device required the invention of two enabling technologies – the CCD flatbed scanner and the text-to-speech synthesizer.\n",
            "On January 13, 1976, the successful finished product was unveiled during a widely reported news conference headed by Kurzweil and the leaders of the National Federation of the Blind.\n",
            "[citation needed] In 1978, Kurzweil Computer Products began selling a commercial version of the optical character recognition computer program.\n",
            "LexisNexis was one of the first customers, and bought the program to upload legal paper and news documents onto its nascent online databases.\n",
            "Two years later, Kurzweil sold his company to Xerox, which had an interest in further commercializing paper-to-computer text conversion.\n",
            "Xerox eventually spun it off as Scansoft, which merged with Nuance Communications.\n",
            "[citation needed] The research group headed by A. G. Ramakrishnan at the Medical intelligence and language engineering lab, Indian Institute of Science, has developed PrintToBraille tool, an open source GUI front end that can be used by any OCR to convert scanned images of printed books to Braille books.\n",
            "In the 2000s, OCR was made available online as a service (WebOCR), in a cloud computing environment, and in mobile applications like real-time translation of foreign-language signs on a smartphone.\n",
            "With the advent of smart-phones and smartglasses, OCR can be used in internet connected mobile device applications that extract text captured using the device's camera.\n",
            "These devices that do not have OCR functionality built into the operating system will typically use an OCR API to extract the text from the image file captured and provided by the device.\n",
            "The OCR API returns the extracted text, along with information about the location of the detected text in the original image back to the device app for further processing (such as text-to-speech) or display.\n",
            "Various commercial and open source OCR systems are available for most common writing systems, including Latin, Cyrillic, Arabic, Hebrew, Indic, Bengali (Bangla), Devanagari, Tamil, Chinese, Japanese, and Korean characters.\n",
            "OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.\n",
            "They can be used for: OCR is generally an \"offline\" process, which analyses a static document.\n",
            "Handwriting movement analysis can be used as input to handwriting recognition.\n",
            "Instead of merely using the shapes of glyphs and words, this technique is able to capture motions, such as the order in which segments are drawn, the direction, and the pattern of putting the pen down and lifting it.\n",
            "This additional information can make the end-to-end process more accurate.\n",
            "This technology is also known as \"on-line character recognition\", \"dynamic character recognition\", \"real-time character recognition\", and \"intelligent character recognition\".\n",
            "OCR software often \"pre-processes\" images to improve the chances of successful recognition.\n",
            "Techniques include: Segmentation of fixed-pitch fonts is accomplished relatively simply by aligning the image to a uniform grid based on where vertical grid lines will least often intersect black areas.\n",
            "For proportional fonts, more sophisticated techniques are needed because whitespace between letters can sometimes be greater than that between words, and vertical lines can intersect more than one character.\n",
            "There are two basic types of core OCR algorithm, which may produce a ranked list of candidate characters.\n",
            "Matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis; it is also known as \"pattern matching\", \"pattern recognition\", or \"image correlation\".\n",
            "This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale.\n",
            "This technique works best with typewritten text and does not work well when new fonts are encountered.\n",
            "This is the technique the early physical photocell-based OCR implemented, rather directly.\n",
            "Feature extraction decomposes glyphs into \"features\" like lines, closed loops, line direction, and line intersections.\n",
            "The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient.\n",
            "These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes.\n",
            "General techniques of feature detection in computer vision are applicable to this type of OCR, which is commonly seen in \"intelligent\" handwriting recognition and indeed most modern OCR software.\n",
            "Nearest neighbour classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match.\n",
            "Software such as Cuneiform and Tesseract use a two-pass approach to character recognition.\n",
            "The second pass is known as \"adaptive recognition\" and uses the letter shapes recognized with high confidence on the first pass to recognize better the remaining letters on the second pass.\n",
            "This is advantageous for unusual fonts or low-quality scans where the font is distorted (e.g.\n",
            "blurred or faded).\n",
            "The OCR result can be stored in the standardized ALTO format, a dedicated XML schema maintained by the United States Library of Congress.\n",
            "For a list of optical character recognition software see Comparison of optical character recognition software.\n",
            "OCR accuracy can be increased if the output is constrained by a lexicon – a list of words that are allowed to occur in a document.\n",
            "This might be, for example, all the words in the English language, or a more technical lexicon for a specific field.\n",
            "This technique can be problematic if the document contains words not in the lexicon, like proper nouns.\n",
            "Tesseract uses its dictionary to influence the character segmentation step, for improved accuracy.\n",
            "The output stream may be a plain text stream or file of characters, but more sophisticated OCR systems can preserve the original layout of the page and produce, for example, an annotated PDF that includes both the original image of the page and a searchable textual representation.\n",
            "\"Near-neighbor analysis\" can make use of co-occurrence frequencies to correct errors, by noting that certain words are often seen together.\n",
            "For example, \"Washington, D.C.\" is generally far more common in English than \"Washington DOC\".\n",
            "Knowledge of the grammar of the language being scanned can also help determine if a word is likely to be a verb or a noun, for example, allowing greater accuracy.\n",
            "The Levenshtein Distance algorithm has also been used in OCR post-processing to further optimize results from an OCR API.\n",
            "In recent years,[when?]\n",
            "the major OCR technology providers began to tweak OCR systems to deal more efficiently with specific types of input.\n",
            "Beyond an application-specific lexicon, better performance may be had by taking into account business rules, standard expression,[clarification needed] or rich information contained in color images.\n",
            "This strategy is called \"Application-Oriented OCR\" or \"Customized OCR\", and has been applied to OCR of license plates, invoices, screenshots, ID cards, driver licenses, and automobile manufacturing.\n",
            "The New York Times has adapted the OCR technology into a proprietary tool they entitle, Document Helper, that enables their interactive news team to accelerate the processing of documents that need to be reviewed.\n",
            "They note that it enables them to process what amounts to as many as 5,400 pages per hour in preparation for reporters to review the contents.\n",
            "There are several techniques for solving the problem of character recognition by means other than improved OCR algorithms.\n",
            "Special fonts like OCR-A, OCR-B, or MICR fonts, with precisely specified sizing, spacing, and distinctive character shapes, allow a higher accuracy rate during transcription in bank check processing.\n",
            "Ironically however, several prominent OCR engines were designed to capture text in popular fonts such as Arial or Times New Roman, and are incapable of capturing text in these fonts that are specialized and much different from popularly used fonts.\n",
            "As Google Tesseract can be trained to recognize new fonts, it can recognize OCR-A, OCR-B and MICR fonts .\n",
            "\"Comb fields\" are pre-printed boxes that encourage humans to write more legibly – one glyph per box.\n",
            "These are often printed in a \"dropout color\" which can be easily removed by the OCR system.\n",
            "Palm OS used a special set of glyphs, known as \"Graffiti\" which are similar to printed English characters but simplified or modified for easier recognition on the platform's computationally limited hardware.\n",
            "Users would need to learn how to write these special glyphs.\n",
            "Zone-based OCR restricts the image to a specific part of a document.\n",
            "This is often referred to as \"Template OCR\".\n",
            "Crowdsourcing humans to perform the character recognition can quickly process images like computer-driven OCR, but with higher accuracy for recognizing images than is obtained with computers.\n",
            "Practical systems include the Amazon Mechanical Turk and reCAPTCHA.\n",
            "The National Library of Finland has developed an online interface for users to correct OCRed texts in the standardized ALTO format.\n",
            "Crowd sourcing has also been used not to perform character recognition directly but to invite software developers to develop image processing algorithms, for example, through the use of rank-order tournaments.\n",
            "Commissioned by the U.S. Department of Energy (DOE), the Information Science Research Institute (ISRI) had the mission to foster the improvement of automated technologies for understanding machine printed documents, and it conducted the most authoritative of the Annual Test of OCR Accuracy from 1992 to 1996.\n",
            "Recognition of Latin-script, typewritten text is still not 100% accurate even where clear imaging is available.\n",
            "One study based on recognition of 19th- and early 20th-century newspaper pages concluded that character-by-character OCR accuracy for commercial OCR software varied from 81% to 99%; total accuracy can be achieved by human review or Data Dictionary Authentication.\n",
            "Other areas—including recognition of hand printing, cursive handwriting, and printed text in other scripts (especially those East Asian language characters which have many strokes for a single character)—are still the subject of active research.\n",
            "The MNIST database is commonly used for testing systems' ability to recognise handwritten digits.\n",
            "Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate.\n",
            "For example, if word context (basically a lexicon of words) is not used to correct software finding non-existent words, a character error rate of 1% (99% accuracy) may result in an error rate of 5% (95% accuracy) or worse if the measurement is based on whether each whole word was recognized with no incorrect letters.\n",
            "An example of the difficulties inherent in digitizing old text is the inability of OCR to differentiate between the \"long s\" and \"f\" characters.\n",
            "Web-based OCR systems for recognizing hand-printed text on the fly have become well known as commercial products in recent years[when?]\n",
            "(see Tablet PC history).\n",
            "Accuracy rates of 80% to 90% on neat, clean hand-printed characters can be achieved by pen computing software, but that accuracy rate still translates to dozens of errors per page, making the technology useful only in very limited applications.\n",
            "[citation needed] Recognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text.\n",
            "Higher rates of recognition of general cursive script will likely not be possible without the use of contextual or grammatical information.\n",
            "For example, recognizing entire words from a dictionary is easier than trying to parse individual characters from script.\n",
            "Reading the Amount line of a cheque (which is always a written-out number) is an example where using a smaller dictionary can increase recognition rates greatly.\n",
            "The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.\n",
            "[citation needed] Most programs allow users to set \"confidence rates\".\n",
            "This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review.\n",
            "Characters to support OCR were added to the Unicode Standard in June 1993, with the release of version 1.1.\n",
            "Some of these characters are mapped from fonts specific to MICR, OCR-A or OCR-B.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJwe3KRZV8j6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMS9B3NfWrjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2count = {}\n",
        "for word in nltk.word_tokenize(clean_text):\n",
        "  if word not in stop_words:\n",
        "    if word not in word2count.keys():\n",
        "      word2count[word] = 1\n",
        "    else:\n",
        "      word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwPP9GAYX3Uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pApC2VWhY0RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key in word2count.keys():\n",
        "  word2count[key] = word2count[key]/max(word2count.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "molIIQgRZf-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent2score = {}\n",
        "for sentence in sentences:\n",
        "  for word in nltk.word_tokenize(sentence.lower()):\n",
        "    if len(sentence.split(' ')) <= 25:\n",
        "      if word in word2count.keys():\n",
        "        if sentence not in sent2score.keys():\n",
        "          sent2score[sentence] = word2count[word]\n",
        "        else:\n",
        "          sent2score[sentence] += word2count[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQmvPBSMZi7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sent2score\n",
        "import heapq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctq64EQcciMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_sentences = heapq.nlargest(5,sent2score,key=sent2score.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJNUk2nFeZQP",
        "colab_type": "code",
        "outputId": "1dff25e0-2738-404f-98c9-9bded3376b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "best_sentences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The shapes of individual cursive characters themselves simply do not contain enough information to accurately (greater than 98%) recognize all handwritten cursive script.',\n",
              " 'This means that if the software does not achieve their desired level of accuracy, a user can be notified for manual review.',\n",
              " 'OCR engines have been developed into many kinds of domain-specific OCR applications, such as receipt OCR, invoice OCR, check OCR, legal billing document OCR.',\n",
              " 'Accuracy rates can be measured in several ways, and how they are measured can greatly affect the reported accuracy rate.',\n",
              " '[citation needed] Recognition of cursive text is an active area of research, with recognition rates even lower than that of hand-printed text.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPUopsfSeibv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}